{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Copy of text_classification_Keras_HS_offiensive2020.ipynb","provenance":[{"file_id":"1KLqCDh2qbdkCUoTM9x0RmaCNGjAb1XpS","timestamp":1605434443274}],"collapsed_sections":[],"mount_file_id":"1aag2eFdMLJYr_hx0inNy0fZAk0jbaDw9","authorship_tag":"ABX9TyMRZ/0ACNEgDnsExh0dxGLd"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"mYJ3KGNAa0Ix"},"source":["import tensorflow as tf\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0fdOKnwDrabZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14210,"status":"ok","timestamp":1605453686627,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"},"user_tz":-120},"outputId":"dd09b4c5-d469-462a-fe0b-9f3e4ae94936"},"source":["!cat '/content/drive/My Drive/MasterThesis/Datasets/Offensive2020/train/HS/1002.txt'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["RT @USER: Ø®Ù„Ø§Øµ ÙŠØ§ Ø§Ø³ØªÙŠØ¹Ø§Ø¨ÙŠ ÙŠØ§ Ø¬Ø²Ù…Ù‡ Ø¨ÙƒØ±Ù‡ Ø§Ø®ØªØ¨Ø§Ø± deal with itØ§ÙŠØ´ Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø¥Ø¬Ø§Ø²Ù‡ Ù‡Ø°ÙŠ Ø®Ù„Ø§Øµ Ø¹Ø§Ø¯\tOFF\tNOT_HS\r\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gfRAxzRxLxcC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":47954,"status":"ok","timestamp":1605453722735,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"},"user_tz":-120},"outputId":"ce18394c-cb2c-44b1-de8e-ecca169ead47"},"source":["batch_size = 32\n","raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n","    \"/content/drive/My Drive/MasterThesis/Datasets/Offensive2020/train\",\n","    batch_size=batch_size,\n","    validation_split=0.2,\n","    subset=\"training\",\n","    seed=1337,\n",")\n","raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n","    \"/content/drive/My Drive/MasterThesis/Datasets/Offensive2020/dev\",\n","    batch_size=batch_size,\n","    validation_split=0.2,\n","    subset=\"validation\",\n","    seed=1337,\n",")\n","raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n","    \"/content/drive/My Drive/MasterThesis/Datasets/Offensive2020/test\", batch_size=batch_size\n",")\n","\n","print(\n","    \"Number of batches in raw_train_ds: %d\"\n","    % tf.data.experimental.cardinality(raw_train_ds)\n",")\n","print(\n","    \"Number of batches in raw_val_ds: %d\" % tf.data.experimental.cardinality(raw_val_ds)\n",")\n","print(\n","    \"Number of batches in raw_test_ds: %d\"\n","    % tf.data.experimental.cardinality(raw_test_ds)\n",")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 7000 files belonging to 2 classes.\n","Using 5600 files for training.\n","Found 7000 files belonging to 2 classes.\n","Using 1400 files for validation.\n","Found 3470 files belonging to 2 classes.\n","Number of batches in raw_train_ds: 175\n","Number of batches in raw_val_ds: 44\n","Number of batches in raw_test_ds: 109\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ydTH5bCXK_kS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":174157,"status":"ok","timestamp":1605453854041,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"},"user_tz":-120},"outputId":"eaf08627-8281-4df5-bb22-561aea2716fc"},"source":["# It's important to take a look at your raw data to ensure your normalization\n","# and tokenization will work as expected. We can do that by taking a few\n","# examples from the training set and looking at them.\n","# This is one of the places where eager execution shines:\n","# we can just evaluate these tensors using .numpy()\n","# instead of needing to evaluate them in a Session/Graph context.\n","for text_batch, label_batch in raw_train_ds.take(1):\n","    for i in range(5):\n","        print(text_batch.numpy()[i].decode('utf-8').strip())\n","        print(label_batch.numpy()[i])\n","        print('--------------------------------')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Ø¹Ù†Ø¯ÙŠ Ù…Ø´ÙƒÙ„Ø© ÙƒØ¨ÙŠØ±Ø© Ù…Ø¹ Ø§Ù„Ù†Ø§Ø³ Ø§Ù„Ù„ÙŠ ØªØ´Ø±Ø­Ù„Ù‡Ø§Ø§ Ø§Ù†Øª Ø§ÙŠÙ‡ ÙˆØ¹ÙŠÙˆØ¨Ùƒ Ø§ÙŠÙ‡ ÙˆÙ…Ø§Ø¨ØªØ­Ø¨Ø´ Ø§ÙŠÙ‡ ÙØ§ÙŠØ¹Ù…Ù„ÙˆØ§ ÙƒÙ„ Ø§Ù„Ù„ÙŠ Ø§Ù†Øª Ø­Ø°Ø±ØªÙ‡Ù… Ù…Ù†Ù‡ ÙˆÙ…Ø§ÙŠØªØ®ÙŠÙ„ÙˆØ´ Ø±Ø¯ ÙØ¹Ù„Ùƒ ÙˆÙŠÙ‚ÙˆÙ„ÙˆØ§ Ø§ØªØµØ¯Ù…Ù†Ø§Ø§ ÙÙŠÙƒØŒ ÙŠØ§Ø¹Ø§Ù„Ù… ÙŠØ§ Ø¬Ø²Ù… ÙŠØ§ ÙˆÙ„Ø§Ø¯ Ø§Ù„Ø§ÙØ§Ø¹ÙŠ Ø§ÙˆÙ…Ø§Ù„ Ø§Ù†Ø§Ø§ ÙƒÙ†Øª Ø¨Ø´Ø±Ø­ ÙÙŠ Ø§ÙŠÙ‡ Ù…Ù† Ø§Ù„Ø§ÙˆÙ„\tOFF\tNOT_HS\n","0\n","--------------------------------\n","@USER Ø¹Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø´ ÙŠØ§ Ù…ÙŠØ±ÙˆÙˆÙˆ ÙŠØ§ Ù…ÙŠØ±Ø§ ÙŠØ§ Ù…ÙŠØ±ØªØ§Ø§Ø§Ø§Ø§ Ø§Ø­Ø¨Ùƒâ¤ï¸â¤ï¸â¤ï¸ğŸ˜­\tNOT_OFF\tNOT_HS\n","1\n","--------------------------------\n","Ø§Ù„Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§Ù„Ø¨Ù„Ø¯ Ø¯ÙŠ Ø¥Ù†Ùƒ Ù„Ùˆ Ù‚Ù„Øª Ù„Ù„Ø­Ù„Ùˆ ÙŠØ§ Ø­Ù„Ùˆ ÙÙŠ Ø¹ÙŠÙˆÙ†Ù‡ ÙŠØ§ Ø¥Ù…Ø§ Ø¨ÙŠØ§Ø®Ø¯ Ù‚Ù„Ù… ÙÙŠ Ù†ÙØ³Ù‡ Ø£Ùˆ Ø¨ÙŠØ®Ø²Ù‚Ù„Ùƒ Ø¹ÙŠÙˆÙ†Ùƒ..!\tNOT_OFF\tNOT_HS\n","1\n","--------------------------------\n","#Ø§Ø·Ù…Ù†_Ø§Ù†Øª_Ù…Ø´_Ù„ÙˆØ­Ø¯Ùƒ<LF>Ø§Ù„Ø®Ø±ÙØ§Ù† Ø§Ù†ØªØ®Øª Ø§Ù„ÙŠ Ø§Ù„Ø§Ø¨Ø¯<LF>Ø­Ø§Ø³Ø³ Ø¨ÙŠÙƒ ÙŠØ§ Ù…Ù‡ØªØ² ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ù…Ø·..Ø±Ø±.Ø§Ù„Ø§Ù‚Ø¨Ø§Ù„ Ø§Ù„Ù†Ù‡Ø§Ø±Ø¯Ø© ÙƒØ§Ù† ØªØ§Ø±ÙŠØ®ÙŠ Ù„Ø§ Ù†ÙØ¹ Ù…Ø¹Ø§Ù‡Ù… Ø§Ø·Ù…Ù† Ùˆ Ù„Ø§ Ø¨Ù„Ø§Ù„ÙŠÙ† Ùˆ Ù„Ø§ ÙƒØªØ§Ø¨Ø© Ø¹Ù„ÙŠ Ø§Ù„Ø­ÙŠØ·Ø§Ù† Ùˆ Ù„Ø§ Ø§Ù„ÙÙ„ÙˆØ³ Ùˆ Ù„Ø§ Ø§Ù„Ø­Ù„Ù„ Ùˆ Ù„Ø§ Ø§Ù„ØºØ·ÙŠØ§Ù† <LF>ÙŠØ§ Ù‚Ù‡Ø±ØªÙƒ ÙŠØ§ Ù…Ù‡ØªØ²<LF>ÙŠØ§ ÙØ¶ÙŠØ­ØªÙƒÙ… ÙŠØ§ Ø®Ø±ÙØ§Ù†<LF>Ø§Ù„Ø®Ø±ÙØ§Ù† Ù‡ÙŠØªØ¬Ù†Ù†Ùˆ <LF>Ø§Ù„Ø³ÙŠØ³ÙŠ Ù‡ÙŠÙ†ÙØ®ÙƒÙ… Ù¡Ù  Ø³Ù†ÙŠÙ† Ø§Ù†ÙŠ Ù‡Ù‡Ù‡Ù‡\tOFF\tHS\n","0\n","--------------------------------\n","@USER ÙŠØ§ Ø²ÙŠØ¯ÙŠ ÙŠØ§ Ø²ÙŠØ¯ÙŠ <LF>Ø§ÙŠÙ‡ Ø§Ù„Ø´Ø¹Ø± Ø¯Ù‡ Ø§Ù„Ù…ØªÙ†Ø¨ÙŠ ÙÙŠ Ø²Ù…Ø§Ù†Ù‡<LF>Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡\tNOT_OFF\tNOT_HS\n","1\n","--------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QxrhvJ1MBjhm","colab":{"background_save":true}},"source":["from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n","import string\n","import re\n","\n","def custom_standardization(input_data):\n","    lowercase = tf.strings.lower(input_data)\n","    stripped_html = tf.strings.regex_replace(lowercase, \"<LF>\", \" \")\n","    return tf.strings.regex_replace(\n","        stripped_html, \"[%s]\" % re.escape(string.punctuation), \"\"\n","    )\n","\n","max_features = 20000\n","embedding_dim = 128\n","sequence_length = 500\n","\n","\n","vectorize_layer = TextVectorization(\n","    standardize=custom_standardization,\n","    max_tokens=max_features,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length,\n",")\n","\n","\n","text_ds = raw_train_ds.map(lambda x, y: x)\n","vectorize_layer.adapt(text_ds)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VwhbmPnsi4S-","colab":{"background_save":true}},"source":["def vectorize_text(text, label):\n","    text = tf.expand_dims(text, -1)\n","    return vectorize_layer(text), label\n","\n","\n","# Vectorize the data.\n","train_ds = raw_train_ds.map(vectorize_text)\n","val_ds = raw_val_ds.map(vectorize_text)\n","test_ds = raw_test_ds.map(vectorize_text)\n","\n","# Do async prefetching / buffering of the data for best performance on GPU.\n","train_ds = train_ds.cache().prefetch(buffer_size=10)\n","val_ds = val_ds.cache().prefetch(buffer_size=10)\n","test_ds = test_ds.cache().prefetch(buffer_size=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xv-M7bb6khq1"},"source":["## Build a model "]},{"cell_type":"code","metadata":{"id":"WCohoIbMhVj9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a476b4ec-10cb-49f9-cc83-08929795507d"},"source":["from tensorflow.keras import layers\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(max_features, 64),\n","    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n","    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n","    tf.keras.layers.Dense(64, activation='relu'),\n","    tf.keras.layers.Dropout(0.5),\n","    tf.keras.layers.Dense(1)\n","])\n","model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","              optimizer=tf.keras.optimizers.Adam(1e-4),\n","              metrics=['accuracy'])\n","history = model.fit(train_ds, epochs=10,\n","                    validation_data=val_ds,\n","                    validation_steps=30)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","175/175 [==============================] - ETA: 0s - loss: 0.5485 - accuracy: 0.6164"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yUxWhjJRkvX2"},"source":["## Train the model"]},{"cell_type":"code","metadata":{"id":"6f40euEzkly4"},"source":["# epochs = 3\n","\n","# # Fit the model using the train and test datasets.\n","# history = model.fit(train_ds, validation_data=val_ds, epochs=epochs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"edg-1dgVk31a"},"source":["## Evaluate the model on the test set\n"]},{"cell_type":"code","metadata":{"id":"LpyFcJNDlTa_"},"source":["test_loss, test_acc = model.evaluate(test_ds)\n","\n","print('Test Loss: {}'.format(test_loss))\n","print('Test Accuracy: {}'.format(test_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B83gmSdHlAAJ"},"source":["## plotlib"]},{"cell_type":"code","metadata":{"id":"HKvke9H8k-DJ"},"source":["import matplotlib.pyplot as plt\n","acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss=history.history['loss']\n","val_loss=history.history['val_loss']\n","\n","#epochs_range = range(22)\n","\n","plt.figure(figsize=(15, 15))\n","plt.subplot(1, 2, 1)\n","plt.plot(acc, label='Training Accuracy')\n","plt.plot(val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(loss, label='Training Loss')\n","plt.plot(val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hKOwfY9olYJG"},"source":[""],"execution_count":null,"outputs":[]}]}
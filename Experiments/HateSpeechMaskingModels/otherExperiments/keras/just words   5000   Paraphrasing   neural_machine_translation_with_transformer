{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"just words | 5000 | Paraphrasing | neural_machine_translation_with_transformer","provenance":[{"file_id":"1WQn_jy0reyPrgoybw0zaFDa3_iS5DzAu","timestamp":1630098377011},{"file_id":"1AKMMMWo52uqXoLjFRwsPUPaLcYxYuuWR","timestamp":1630059224425},{"file_id":"16_nfzINfxjl0wXe7OmHg3R1YLoR-mcnp","timestamp":1629471209050},{"file_id":"1AKVcu4C08_OWa07Ghl84q6WmpijlrClk","timestamp":1629307981327},{"file_id":"1u2x4I-WagUER4QJNd4tt0bkz8ilfnDly","timestamp":1628998157816},{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/nlp/ipynb/neural_machine_translation_with_transformer.ipynb","timestamp":1628672106359}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"cells":[{"cell_type":"markdown","metadata":{"id":"OoI1jDpua-wl"},"source":["# English-to-Spanish translation with a sequence-to-sequence Transformer\n","\n","**Author:** [fchollet](https://twitter.com/fchollet)<br>\n","**Date created:** 2021/05/26<br>\n","**Last modified:** 2021/05/26<br>\n","**Description:** Implementing a sequence-to-sequene Transformer and training it on a machine translation task."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bTHMBYKRbi1T","executionInfo":{"status":"ok","timestamp":1630098688919,"user_tz":-180,"elapsed":47665,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}},"outputId":"7d106ce4-c99a-4426-8666-eabf5a031586"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QC6rAuTna-wp"},"source":["## Introduction\n","\n","In this example, we'll build a sequence-to-sequence Transformer model, which\n","we'll train on an English-to-Spanish machine translation task.\n","\n","You'll learn how to:\n","\n","- Vectorize text using the Keras `TextVectorization` layer.\n","- Implement a `TransformerEncoder` layer, a `TransformerDecoder` layer,\n","and a `PositionalEmbedding` layer.\n","- Prepare data for training a sequence-to-sequence model.\n","- Use the trained model to generate translations of never-seen-before\n","input sentences (sequence-to-sequence inference).\n","\n","The code featured here is adapted from the book\n","[Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition)\n","(chapter 11: Deep learning for text).\n","The present example is fairly barebones, so for detailed explanations of\n","how each building block works, as well as the theory behind Transformers,\n","I recommend reading the book."]},{"cell_type":"markdown","metadata":{"id":"7il3-WZPa-wr"},"source":["## Setup"]},{"cell_type":"code","metadata":{"id":"ctfudCVLVpA6"},"source":["# !pip install --upgrade tensorflow\n","# !pip install --upgrade tensorflow-gpu"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mxIpmPJfa-ws","executionInfo":{"status":"ok","timestamp":1630098692712,"user_tz":-180,"elapsed":3798,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["import pathlib\n","import random\n","import string\n","import re\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6WkEVy-Za-wt"},"source":["## Downloading the data\n","\n","We'll be working with an English-to-Spanish translation dataset\n","provided by [Anki](https://www.manythings.org/anki/). Let's download it:"]},{"cell_type":"code","metadata":{"id":"W8FYDJLEa-wu","executionInfo":{"status":"ok","timestamp":1630098722970,"user_tz":-180,"elapsed":380,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["# text_file = keras.utils.get_file(\n","#     fname=\"spa-eng.zip\",\n","#     origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n","#     extract=True,\n","# )\n","# text_file = pathlib.Path('/content/drive/MyDrive/MasterThesis/paraphrasing/ParapgrasingMask/Parapgrasing - Masking - maskWithWords.tsv')\n","# text_file = pathlib.Path('/content/drive/MyDrive/MasterThesis/paraphrasing/NOTHS_Parapgrasing - Masking - MaskWithoutWords.tsv')\n","text_file = pathlib.Path('/content/drive/MyDrive/MasterThesis/paraphrasing/maskbadWords.tsv')\n","# text_file = pathlib.Path('/content/drive/MyDrive/MasterThesis/paraphrasing/ParapgrasingMask/ParapgrasingMasking.tsv')"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3GIGj5-Qa-wv"},"source":["## Parsing the data\n","\n","Each line contains an English sentence and its corresponding Spanish sentence.\n","The English sentence is the *source sequence* and Spanish one is the *target sequence*.\n","We prepend the token `\"[start]\"` and we append the token `\"[end]\"` to the Spanish sentence."]},{"cell_type":"code","metadata":{"id":"TemcqrMsa-ww","executionInfo":{"status":"ok","timestamp":1630098730838,"user_tz":-180,"elapsed":2456,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["with open(text_file) as f:\n","    lines = f.read().split(\"\\n\")[:-1]\n","text_pairs = []\n","for line in lines:\n","    inp, targ = line.split(\"\\t\")\n","    targ = \"[start] \" + targ + \" [end]\"\n","    text_pairs.append((inp, targ))"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uVT6PBMga-wx"},"source":["Here's what our sentence pairs look like:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RVHW_4Gva-wy","executionInfo":{"status":"ok","timestamp":1630098730840,"user_tz":-180,"elapsed":7,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}},"outputId":"7d54e330-0dcf-44f6-e120-41c0b947bb3e"},"source":["for _ in range(5):\n","    print(random.choice(text_pairs))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["('خاينة', '[start] ***** [end]')\n","('سامجة', '[start] ***** [end]')\n","('فاغر', '[start] **** [end]')\n","('يهودي', '[start] ***** [end]')\n","('اموتكم', '[start] ****** [end]')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Z5tRhKiMa-wz"},"source":["Now, let's split the sentence pairs into a training set, a validation set,\n","and a test set."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8mUPXPWYa-wz","executionInfo":{"status":"ok","timestamp":1630098737006,"user_tz":-180,"elapsed":801,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}},"outputId":"485abc9c-798a-4e52-ff9b-dfb9a056205c"},"source":["random.shuffle(text_pairs)\n","num_val_samples = int(0.25 * len(text_pairs))\n","num_train_samples = len(text_pairs) -  num_val_samples\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n","# test_pairs = text_pairs[num_train_samples + num_val_samples :]\n","\n","print(f\"{len(text_pairs)} total pairs\")\n","print(f\"{len(train_pairs)} training pairs\")\n","print(f\"{len(val_pairs)} validation pairs\")\n","# print(f\"{len(test_pairs)} test pairs\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["1448 total pairs\n","1086 training pairs\n","362 validation pairs\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IfI1L6L2l-bh","executionInfo":{"status":"ok","timestamp":1630098737620,"user_tz":-180,"elapsed":4,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}},"outputId":"8bba5bc4-e4ce-40e9-f77d-df9e43985b5a"},"source":["test_file = pathlib.Path('/content/drive/MyDrive/MasterThesis/paraphrasing/paraphrasingTest.tsv')\n","with open(test_file) as f:\n","    lines = f.read().split(\"\\n\")[:-1]\n","test_pairs = []\n","for line in lines:\n","    inp, targ = line.split(\"\\t\")\n","    targ = \"[start] \" + targ + \" [end]\"\n","    test_pairs.append((inp, targ))\n","\n","for _ in range(5):\n","    print(random.choice(test_pairs))\n","\n","random.shuffle(test_pairs)\n","num_test_samples = len(test_pairs)\n","test_pairs = test_pairs[: num_test_samples]\n","print(f\"{len(test_pairs)} test pairs\")"],"execution_count":7,"outputs":[{"output_type":"stream","text":["('جايكم جايكم يا السعوديين يا الكبسه كلها يوم ونص بس', '[start] جايكم جايكم يا السعوديين يا الكبسه كلها يوم ونص بس [end]')\n","('يا ابن الكافرة يا رونالدو ... تحرك سم', '[start] يا ابن ******* يا رونالدو ... تحرك ** [end]')\n","('يا جماعة يا جدعان يا شباب يا محترمين يا اساتذة يا ولاد الناس يا ولاد الوسخة مًـيُـسي لا يقارن 🖤👑 …', '[start] يا جماعة يا جدعان يا شباب يا محترمين يا اساتذة يا ولاد الناس يا ولاد ****** مًـيُـسي لا يقارن 🖤👑 … [end]')\n","('يا تيمو يا دولي عقبال ما افرح بتخرجك كدة🔥🔥🔥🔥 بحبك فشخ يسطاا يا فخر المنصورة ومصر كلها♥️♥️♥️♥️', '[start] يا تيمو يا دولي عقبال ما افرح بتخرجك كدة🔥🔥🔥🔥 بحبك *** يسطاا يا فخر المنصورة ومصر كلها♥️♥️♥️♥️ [end]')\n","('يا كافر يا زنديق يا مرتد يا انت عاوز يبقى عندنا ديمقراطية زى الكفرة اللى ما يعرفوش ربنا', '[start] يا **** يا ***** يا **** يا انت عاوز يبقى عندنا ديمقراطية زى ****** اللى ما يعرفوش ربنا [end]')\n","401 test pairs\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1CpqzqNaa-w0"},"source":["## Vectorizing the text data\n","\n","We'll use two instances of the `TextVectorization` layer to vectorize the text\n","data (one for English and one for Spanish),\n","that is to say, to turn the original strings into integer sequences\n","where each integer represents the index of a word in a vocabulary.\n","\n","The English layer will use the default string standardization (strip punctuation characters)\n","and splitting scheme (split on whitespace), while\n","the Spanish layer will use a custom standardization, where we add the character\n","`\"¿\"` to the set of punctuation characters to be stripped.\n","\n","Note: in a production-grade machine translation model, I would not recommend\n","stripping the punctuation characters in either language. Instead, I would recommend turning\n","each punctuation character into its own token,\n","which you could achieve by providing a custom `split` function to the `TextVectorization` layer."]},{"cell_type":"code","metadata":{"id":"CPoQesZea-w0","executionInfo":{"status":"ok","timestamp":1630098756975,"user_tz":-180,"elapsed":1602,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["# strip_chars = string.punctuation + \"¿\"\n","strip_chars = \"[a-zA-Z]|\\d+|[٠١٢٣٤٥٦٧٨٩]|[.#،<>@,\\\\-_”“٪ًَ]\"\n","\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n","\n","vocab_size = 1450\n","sequence_length = 20\n","batch_size = 64\n","\n","\n","def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)\n","    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n","\n","\n","inp_vectorization = TextVectorization(\n","    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",")\n","targ_vectorization = TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length + 1,\n","    standardize=custom_standardization,\n",")\n","train_eng_texts = [pair[0] for pair in train_pairs]\n","train_spa_texts = [pair[1] for pair in train_pairs]\n","inp_vectorization.adapt(train_eng_texts)\n","targ_vectorization.adapt(train_spa_texts)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8enIADXMa-w1"},"source":["Next, we'll format our datasets.\n","\n","At each training step, the model will seek to predict target words N+1 (and beyond)\n","using the source sentence and the target words 0 to N.\n","\n","As such, the training dataset will yield a tuple `(inputs, targets)`, where:\n","\n","- `inputs` is a dictionary with the keys `encoder_inputs` and `decoder_inputs`.\n","`encoder_inputs` is the vectorized source sentence and `encoder_inputs` is the target sentence \"so far\",\n","that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target sentence.\n","- `target` is the target sentence offset by one step:\n","it provides the next words in the target sentence -- what the model will try to predict."]},{"cell_type":"code","metadata":{"id":"dKmCVA5Ka-w2","executionInfo":{"status":"ok","timestamp":1630098760991,"user_tz":-180,"elapsed":507,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["\n","def format_dataset(inp, targ):\n","    inp = inp_vectorization(inp)\n","    targ = targ_vectorization(targ)\n","    return ({\"encoder_inputs\": inp, \"decoder_inputs\": targ[:, :-1],}, targ[:, 1:])\n","\n","\n","def make_dataset(pairs):\n","    inp_texts, targ_texts = zip(*pairs)\n","    inp_texts = list(inp_texts)\n","    targ_texts = list(targ_texts)\n","    dataset = tf.data.Dataset.from_tensor_slices((inp_texts, targ_texts))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset)\n","    return dataset.shuffle(248).prefetch(16).cache()\n","\n","\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U6ZQPBz4a-w3"},"source":["Let's take a quick look at the sequence shapes\n","(we have batches of 64 pairs, and all sequences are 20 steps long):"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PkVSFSxBa-w3","executionInfo":{"status":"ok","timestamp":1630098766084,"user_tz":-180,"elapsed":891,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}},"outputId":"5f253204-aabe-45e7-be27-1ecb96064ca8"},"source":["for inputs, targets in train_ds.take(1):\n","    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n","    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n","    print(f\"targets.shape: {targets.shape}\")"],"execution_count":10,"outputs":[{"output_type":"stream","text":["inputs[\"encoder_inputs\"].shape: (64, 20)\n","inputs[\"decoder_inputs\"].shape: (64, 20)\n","targets.shape: (64, 20)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GQ6z9fVea-w3"},"source":["## Building the model\n","\n","Our sequence-to-sequence Transformer consists of a `TransformerEncoder`\n","and a `TransformerDecoder` chained together. To make the model aware of word order,\n","we also use a `PositionalEmbedding` layer.\n","\n","The source sequence will be pass to the `TransformerEncoder`,\n","which will produce a new representation of it.\n","This new representation will then be passed\n","to the `TransformerDecoder`, together with the target sequence so far (target words 0 to N).\n","The `TransformerDecoder` will then seek to predict the next words in the target sequence (N+1 and beyond).\n","\n","A key detail that makes this possible is causal masking\n","(see method `get_causal_attention_mask()` on the `TransformerDecoder`).\n","The `TransformerDecoder` sees the entire sequences at once, and thus we must make\n","sure that it only uses information from target tokens 0 to N when predicting token N+1\n","(otherwise, it could use information from the future, which would\n","result in a model that cannot be used at inference time)."]},{"cell_type":"code","metadata":{"id":"3jHRvQaHa-w3","executionInfo":{"status":"ok","timestamp":1630098766402,"user_tz":-180,"elapsed":6,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["\n","class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super(TransformerEncoder, self).__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.dropout = tf.keras.layers.Dropout(0.5)\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n","        attention_output = self.attention(\n","            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n","        )\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","\n","\n","class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n","        super(PositionalEmbedding, self).__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=vocab_size, output_dim=embed_dim\n","        )\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=embed_dim\n","        )\n","        self.sequence_length = sequence_length\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","\n","\n","class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n","        super(TransformerDecoder, self).__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.latent_dim = latent_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.dropout = tf.keras.layers.Dropout(0.5)\n","\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n","            padding_mask = tf.minimum(padding_mask, causal_mask)\n","\n","        attention_output_1 = self.attention_1(\n","            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n","        )\n","        out_1 = self.layernorm_1(inputs + attention_output_1)\n","\n","        attention_output_2 = self.attention_2(\n","            query=out_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","        )\n","        out_2 = self.layernorm_2(out_1 + attention_output_2)\n","\n","        proj_output = self.dense_proj(out_2)\n","        return self.layernorm_3(out_2 + proj_output)\n","\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n","            axis=0,\n","        )\n","        return tf.tile(mask, mult)\n"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WCD0jPjqa-w4"},"source":["Next, we assemble the end-to-end model."]},{"cell_type":"code","metadata":{"id":"PaxOj5PWa-w5","executionInfo":{"status":"ok","timestamp":1630098767744,"user_tz":-180,"elapsed":1347,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["embed_dim = 256\n","latent_dim = 2048\n","num_heads = 8\n","\n","encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n","encoder = keras.Model(encoder_inputs, encoder_outputs)\n","\n","decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n","encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","# x = layers.Dropout(0.5)(x)\n","\n","x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n","\n","x = layers.Dropout(0.5)(x)\n","decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","# x = layers.Dropout(0.2)(x)\n","\n","decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n","\n","decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n","transformer = keras.Model(\n","    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",")"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9dFxoe3ma-w5"},"source":["## Training our model\n","\n","We'll use accuracy as a quick way to monitor training progress on the validation data.\n","Note that machine translation typically uses BLEU scores as well as other metrics, rather than accuracy.\n","\n","Here we only train for 1 epoch, but to get the model to actually converge\n","you should train for at least 30 epochs."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8tGD8iZLa-w6","executionInfo":{"status":"ok","timestamp":1630098909694,"user_tz":-180,"elapsed":141953,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}},"outputId":"d316ce20-7012-4301-d55d-5201377ae5c6"},"source":["epochs = 30  # This should be at least 30 for convergence\n","my_callbacks = [\n","    tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=2),\n","    # tf.keras.callbacks.ReduceLROnPlateau(\n","    #   monitor='val_loss', factor=0.1, patience=2,\n","    #   min_lr=0,\n","    # )\n","]\n","\n","transformer.summary()\n","transformer.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",")\n","\n","history  = transformer.fit(train_ds, epochs=epochs, validation_data=val_ds,callbacks=my_callbacks)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Model: \"transformer\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","encoder_inputs (InputLayer)     [(None, None)]       0                                            \n","__________________________________________________________________________________________________\n","positional_embedding (Positiona (None, None, 256)    376320      encoder_inputs[0][0]             \n","__________________________________________________________________________________________________\n","decoder_inputs (InputLayer)     [(None, None)]       0                                            \n","__________________________________________________________________________________________________\n","transformer_encoder (Transforme (None, None, 256)    3155456     positional_embedding[0][0]       \n","__________________________________________________________________________________________________\n","model_1 (Functional)            (None, None, 1450)   6008490     decoder_inputs[0][0]             \n","                                                                 transformer_encoder[0][0]        \n","==================================================================================================\n","Total params: 9,540,266\n","Trainable params: 9,540,266\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Epoch 1/30\n","17/17 [==============================] - 32s 2s/step - loss: 0.2363 - accuracy: 0.7179 - val_loss: 0.0884 - val_accuracy: 0.7781\n","Epoch 2/30\n","17/17 [==============================] - 28s 2s/step - loss: 0.0921 - accuracy: 0.7624 - val_loss: 0.0821 - val_accuracy: 0.7689\n","Epoch 3/30\n","17/17 [==============================] - 28s 2s/step - loss: 0.0963 - accuracy: 0.7572 - val_loss: 0.0867 - val_accuracy: 0.7689\n","Epoch 4/30\n","17/17 [==============================] - 28s 2s/step - loss: 0.0561 - accuracy: 0.8874 - val_loss: 0.1367 - val_accuracy: 0.7735\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"id":"AoxqTnCF_2tK","executionInfo":{"status":"ok","timestamp":1630098924856,"user_tz":-180,"elapsed":15182,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}},"outputId":"f3d190c4-4e0a-4892-f73c-20d93539af85"},"source":["import matplotlib.pyplot as plt\n","\n","s, (at, al) = plt.subplots(2,1)\n","at.plot(history.history['accuracy'], c= 'b')\n","at.plot(history.history['val_accuracy'], c='r')\n","at.set_title('model accuracy')\n","at.set_ylabel('accuracy')\n","at.set_xlabel('epoch')\n","at.legend(['train', 'val'], loc='upper left')\n","\n","al.plot(history.history['loss'], c='m')\n","al.plot(history.history['val_loss'], c='c')\n","al.set_title('model loss')\n","al.set_ylabel('loss')\n","al.set_xlabel('epoch')\n","al.legend(['train', 'val'], loc = 'upper left')"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7f28f6498410>"]},"metadata":{},"execution_count":14},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"SL-vwLH1a-w6"},"source":["## Decoding test sentences\n","\n","Finally, let's demonstrate how to translate brand new English sentences.\n","We simply feed into the model the vectorized English sentence\n","as well as the target token `\"[start]\"`, then we repeatedly generated the next token, until\n","we hit the token `\"[end]\"`."]},{"cell_type":"code","metadata":{"id":"okeujBqTa-w6","executionInfo":{"status":"ok","timestamp":1630098924857,"user_tz":-180,"elapsed":16,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["spa_vocab = targ_vectorization.get_vocabulary()\n","spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n","max_decoded_sentence_length = 20\n","\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = inp_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = targ_vectorization([decoded_sentence])[:, :-1]\n","        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n","\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = spa_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","\n","        if sampled_token == \"[end]\":\n","            break\n","    return decoded_sentence\n","\n","\n"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"qZlMgW0dnxno","executionInfo":{"status":"ok","timestamp":1630098924857,"user_tz":-180,"elapsed":15,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["# from nltk.translate.bleu_score import corpus_bleu\n","# from nltk.translate.bleu_score import sentence_bleu\n","# predicted_list = []\n","# test_inp_texts = [pair[0] for pair in test_pairs]\n","# test_targ_texts = [pair[1] for pair in test_pairs]\n","\n","# score_list = []\n","# from nltk.translate.bleu_score import SmoothingFunction\n","# # print(len(test_inp_texts))\n","# # print(len(test_targ_texts))\n","# # def bleu_score():\n","\n","# for i,j in zip(test_inp_texts,test_targ_texts):\n","\n","#   input_sentence = test_inp_texts\n","#     # print(input_sentence[i])\n","#   translated = decode_sequence(i)\n","#   predicted = list(translated.split(\",\"))\n","#   score = sentence_bleu(i, translated, weights=(1, 0, 0, 0))\n","#   score_list.append(score)\n","#   predicted_list.append(predicted)\n","#   print(\"Input:\",i,\"\\n Actual\",j,\"\\n Predicted\",translated)\n","#   score = sentence_bleu(i, translated, weights=(1, 0, 0, 0))\n","#   print(\"blue score : \",score,\"\\n\\n\")\n","# avg = sum(score_list) / len(score_list)\n","# print(\"Average of the list =\", round(avg, 2))\n","\n","#   # return bleu_dic\n","\n","\n","# # bleu_score()\n","# # bleu_test"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"hiKXV65HxspA","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1630098957597,"user_tz":-180,"elapsed":32755,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}},"outputId":"c6a16767-3776-4ced-97b6-48f320a100ce"},"source":["from nltk.translate.bleu_score import corpus_bleu\n","from nltk.translate.bleu_score import sentence_bleu\n","predicted_list = []\n","test_inp_texts = [pair[0] for pair in test_pairs]\n","test_targ_texts = [pair[1] for pair in test_pairs]\n","\n","score_list = []\n","score_list_2 = []\n","score_list_3 = []\n","score_list_4 = []\n","from nltk.translate.bleu_score import SmoothingFunction\n","# print(len(test_inp_texts))\n","# print(len(test_targ_texts))\n","# def bleu_score():\n","\n","for i,j in zip(test_inp_texts,test_targ_texts):\n","\n","  input_sentence = test_inp_texts\n","    # print(input_sentence[i])\n","  translated = decode_sequence(i)\n","  predicted = list(translated.split(\",\"))\n","  # score = sentence_bleu(i, translated, weights=(1, 0, 0, 0))\n","  score_1 = sentence_bleu(i, translated, weights=(1, 0, 0, 0))\n","  score_2 = sentence_bleu(i, translated,weights=(0.5, 0.5, 0, 0))\n","  score_3 = sentence_bleu(i, translated, weights=(0.33, 0.33, 0.33, 0))\n","  score_4 = sentence_bleu(i, translated, weights=(0.25, 0.25, 0.25, 0.25))\n","  score_list.append(score_1)\n","  score_list_2.append(score_2)\n","  score_list_3.append(score_3)\n","  score_list_4.append(score_4)\n","  predicted_list.append(predicted)\n","  print(\"Input:\",i,\"\\n Actual\",j,\"\\n Predicted\",translated)\n","  # score_1 = sentence_bleu(i, translated, weights=(1, 0, 0, 0))\n","  # score_2 = sentence_bleu(i, translated,weights=(0.5, 0.5, 0, 0))\n","  # score_3 = sentence_bleu(i, translated, weights=(0.33, 0.33, 0.33, 0))\n","  # score_4 = sentence_bleu(i, translated, weights=(0.25, 0.25, 0.25, 0.25))\n","  print(\"blue 1-gram : \",score_1,\"\\n\",\"blue 2-gram : \",score_2,\"\\n\",\"blue 3-gram : \",score_3,\"\\n\",\"blue 4-gram : \",score_4,\"\\n\")\n","avg = sum(score_list) / len(score_list)\n","avg_2 = sum(score_list_2) / len(score_list_2)\n","avg_3 = sum(score_list_3) / len(score_list_3)\n","avg_4 = sum(score_list_4) / len(score_list_4)\n","print(\"Average of the list 1-gram=\", round(avg, 2),\"Average of the list 2-gram=\", round(avg_2, 2),\"Average of the list 3-gram=\", \n","      round(avg_3, 2),\"Average of the list 4-gram=\", round(avg_4, 2))\n","\n","  # return bleu_dic\n","\n","\n","# bleu_score()\n","# bleu_test"],"execution_count":17,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n","Corpus/Sentence contains 0 counts of 2-gram overlaps.\n","BLEU scores might be undesirable; use SmoothingFunction().\n","  warnings.warn(_msg)\n"],"name":"stderr"},{"output_type":"stream","text":["Input: يا هلاليه يا منافقين فجأه تحبون الاتحاد؟ \n"," Actual [start] يا هلاليه يا ******* فجأه تحبون الاتحاد؟ [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: يا مغفل يا تنكه اقطع واخس وافهم سياق الكلام كنت أتكلم عن كل ملتحي متشدد اخونجي غرضه سلطوي خبيث اقلب وجهك بس وتقلع \n"," Actual [start] يا **** يا **** اقطع واخس وافهم سياق الكلام كنت أتكلم عن كل ملتحي متشدد ****** غرضه سلطوي **** اقلب وجهك بس و**** [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: ولد عمي يا صرصري فالاخير اني بأسمي و صورتي يا وهمي يا مريض برا نيك \n"," Actual [start] ولد عمي يا ***** فالاخير اني بأسمي و صورتي يا **** يا **** برا *** [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: 😂😂😂😂 دعم ايه يا منحوس يا شحات . ولا يمكن الدعم بمعلومات كيفية عمل الانقلاب \n"," Actual [start] 😂😂😂😂 دعم ايه يا ***** يا **** . ولا يمكن الدعم بمعلومات كيفية عمل الانقلاب [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: حمدلله ع سلامتك يا سافله يا سهلة يا رخيصة😂♥️ \n"," Actual [start] حمدلله ع سلامتك يا ***** يا **** يا *****😂♥️ [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: ينعل الذي رباش يا وصخه يا جيفه تف ع وجهك وعاده غاليه عليك يا صندل \n"," Actual [start] **** الذي رباش يا **** يا **** ** ع وجهك وعاده غاليه عليك يا **** [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: الصهاينة انتم يا خونة يا قتلة الأطفال ومخربين لم تتركوا بلد الا وخربتموه بمعية المرتزقة لعنة الله عليكم يا زنادقة انا اعيش في الجزائر 🇩🇿 ولا حاجة لي في مالكم القذر \n"," Actual [start] الصهاينة انتم يا **** يا **** الأطفال و****** لم تتركوا بلد الا وخربتموه بمعية ******** **** الله عليكم يا ****** انا اعيش في الجزائر 🇩🇿 ولا حاجة لي في مالكم ***** [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: يا جروس يا حمارثالت ,,شفت ابراهيم حسن بياكل النجيله ازاي لما حس ان مكانه اتهدد بوجود زيزو ,,اوباما وكهربا عايزين يقعدوا شويه \n"," Actual [start] يا **** يا ****ثالت ,,شفت ابراهيم حسن بياكل النجيله ازاي لما حس ان مكانه اتهدد بوجود زيزو ,,اوباما وكهربا عايزين يقعدوا شويه [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: حريم السلطان امثالك مجرد ذكر سيدهم السيسى يجيلهم تبول لا ارادى انصحك تروح عن ولى نعمتك القردوغان تمارس الشذوذ عنده دا خليفتكم المنشود يا أفاق يا مدلس يا تاجر الدين يا راعى الخراب \n"," Actual [start] حريم السلطان امثالك مجرد ذكر سيدهم السيسى يجيلهم **** لا ارادى انصحك تروح عن ولى نعمتك ********* تمارس ****** عنده دا خليفتكم المنشود يا أفاق يا **** يا تاجر الدين يا راعى ****** [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: Popular opinion: البنطلون ساتر اكتر من الاسكيرت يا بجم يا سجم يا عجم يا نجم \n"," Actual [start] Popular opinion: البنطلون ساتر اكتر من الاسكيرت يا *** يا *** يا *** يا *** [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.11764705882352941 \n"," blue 2-gram :  0.3429971702850177 \n"," blue 3-gram :  0.49350522429506183 \n"," blue 4-gram :  0.5856596027429395 \n","\n","Input: الخليج عربي يا بهيمة وليس فارسي يا مجوسي يا خنزير \n"," Actual [start] الخليج عربي يا ***** وليس فارسي يا ***** يا ***** [end] \n"," Predicted [start] ***** [en]                  \n","blue 1-gram :  0.02777777777777778 \n"," blue 2-gram :  0.16666666666666669 \n"," blue 3-gram :  0.3064927280413241 \n"," blue 4-gram :  0.408248290463863 \n","\n","Input: المهم يعنى اعملو اي حاجه و قولو اى كلام و متجيبوش سيرة الزمالك و لا جمهوره علشان دول انضف منكم بسنين ضوئية و الزمالك انضف و ارفع من التهزيق بتاع ناديكم دا يا مهزقين يا بتوع الخمسة 😂 \n"," Actual [start] المهم يعنى اعملو اي حاجه و قولو اى كلام و متجيبوش سيرة الزمالك و لا جمهوره علشان دول انضف منكم بسنين ضوئية و الزمالك انضف و ارفع من ******* بتاع ناديكم دا يا ****** يا بتوع الخمسة 😂 [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: يا بن عوف يا مسلوب ما بيحكمنا رئيس مطلوب .. #استمرار_اعتصام_القيادة_العامة #تسقط_تاني_بس \n"," Actual [start] يا بن عوف يا ***** ما بيحكمنا رئيس مطلوب .. #استمرار_اعتصام_القيادة_العامة #تسقط_تاني_بس [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: ليش متاكل خرة انتة وعمر ابن الصهال ابن البغايا اشد غيرة من رسول الله صل الله عليه واله وسلم يا قذارة البشر يا يهود الأمة لعنك الله ولعن دينكم القذر ولعنت الله عمر ابن الصهاك يا وابو بقر \n"," Actual [start] ليش متاكل *** انتة وعمر ابن ****** ابن ******* اشد **** من رسول الله صل الله عليه واله وسلم يا ***** البشر يا **** الأمة **** الله و*** دينكم ***** ولعنت الله عمر ابن الصهاك يا وابو *** [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: #قطر_حليفة_الشيطان هي من تساعد اسرائيل وامريكا لتحقيق صفقة القرن يا عبدالله العذبة يا وجه العنزة ولكن مافشرتو طول,مافي قادة امثال,ابوفادي لن يسمح لكم بتحقيق خيانتكم \n"," Actual [start] #قطر_حليفة_الشيطان هي من تساعد اسرائيل وامريكا لتحقيق صفقة القرن يا عبدالله العذبة يا وجه ****** ولكن مافشرتو طول,مافي قادة امثال,ابوفادي لن يسمح لكم بتحقيق ******* [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: طبعا مش هنعرف نعيش من غيرك يا بنت قلبى يا كلبوبه انتى متقوليش كده تانى يا كلبوبه💞💞💞💞💞💞💞💋💋💋💋💋💋 \n"," Actual [start] طبعا مش هنعرف نعيش من غيرك يا بنت قلبى يا ****** **** متقوليش كده تانى يا ******💞💞💞💞💞💞💞💋💋💋💋💋💋 [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: الله يا خذك يا نمق ، يتفلسف وهو مثل الادلخ بس ينفذ اوامر حمد بن جاسم راس الحيه 🤮🤮 \n"," Actual [start] الله ***** يا *** ، ****** وهو مثل ****** بس ينفذ اوامر حمد بن جاسم راس ***** 🤮🤮 [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: خليك فى اكل عيشك يا دلدول يا وسخ وبالمناسبة اخر مره الزمالك فاز بافريقيا كنت انت لسه بتتاخد فى الخرابات 😀 \n"," Actual [start] خليك فى اكل عيشك يا ***** يا *** وبالمناسبة اخر مره الزمالك فاز بافريقيا كنت انت لسه بتتاخد فى ******** 😀 [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: كسمك يا ابن العرص يا اللي امك زانيه اهلي ميين اللي مستواه كده واهلي ميين اللي الخكام بتسانده يا ابن المره الفاجره \n"," Actual [start] **** يا ابن ***** يا اللي امك ***** اهلي ميين اللي مستواه كده واهلي ميين اللي الخكام بتسانده يا ابن المره ******* [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: يعني يا ايهاب جلال يا ابن كس النعجة مصدع ميتين اهالينا بالهجوم وبالخرا وجاي تعملي مورينيو دلوقتي \n"," Actual [start] يعني يا ايهاب جلال يا ابن ** النعجة مصدع ميتين اهالينا بالهجوم وب***** وجاي تعملي مورينيو دلوقتي [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: * انا جايب 60 % وعاوز ابقي مهندس * سيب 8 تلاف جنيه وروح علي معهد تقاوي للهندسه والتكنولوجيا قلهم انا جايلكوا من طرف حاتم باشا * شكرا يا باشا * استني يا بأف فين الفلوووووس #اقفلوا_دكاكين_هندسة \n"," Actual [start] * انا جايب 60 % وعاوز ابقي مهندس * سيب 8 تلاف جنيه وروح علي معهد تقاوي للهندسه والتكنولوجيا قلهم انا جايلكوا من طرف حاتم باشا * شكرا يا باشا * استني يا *** فين الفلوووووس #اقفلوا_دكاكين_هندسة [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.0588235294117647 \n"," blue 2-gram :  0.24253562503633297 \n"," blue 3-gram :  0.392601410850376 \n"," blue 4-gram :  0.4924790605054523 \n","\n","Input: ماهذه الوضاعه يا هوه هل أصبح اتحاد الكرة السعوديه ندا للهلال وينافسه كبير يا هلال كبير يا هلال 💙💙💙💙💙💙💙💙💙💙 … \n"," Actual [start] ماهذه ******* يا هوه هل أصبح اتحاد الكرة السعوديه ندا للهلال وينافسه كبير يا هلال كبير يا هلال 💙💙💙💙💙💙💙💙💙💙 … [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: طب احب انوه للاخوة البهايم اللي هيفهم غلط انا بطبل و بحفل يا متخلفين يا هبل وبس شكرا✋😂 \n"," Actual [start] طب احب انوه للاخوة ******* اللي هيفهم غلط انا **** و بحفل يا ******* يا *** وبس شكرا✋😂 [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: #الاتحاد_النصر الله يحرق الي مخليك تلعب في الاتحاد يا أحمد عسيري يا سبك 😠 \n"," Actual [start] #الاتحاد_النصر الله **** الي مخليك تلعب في الاتحاد يا أحمد عسيري يا *** 😠 [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: اشعر بالعار بالفعل ليس من ريال مدريد بل من ال**و&amp;ا* الذي يدير الفريق لك سولاري بنفس العناصر طعماهم ٣ يا واطي يا منحط يا حقير و لوبتيغي الي بعيونكم فاشل اصقطهم بال٤ يا قذر يا سافل يا وضيع ميركاتو مع مارسيلو رح يكون 💩 ب تم زيدا \n"," Actual [start] اشعر ****** بالفعل ليس من ريال مدريد بل من ال**و&amp;ا* الذي يدير الفريق لك سولاري بنفس العناصر طعماهم ٣ يا **** يا **** يا **** و لوبتيغي الي بعيونكم **** اصقطهم بال٤ يا *** يا **** يا **** ميركاتو مع مارسيلو رح يكون 💩 ب تم زيدا [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.08823529411764706 \n"," blue 2-gram :  0.2970442628930023 \n"," blue 3-gram :  0.4488094280005903 \n"," blue 4-gram :  0.5450176720923848 \n","\n","Input: #تسقط_بس يا سيسي يا بشير يا بن سلمان يا بن زايد يا كل حاكم فاجر ابن وسخة مستبد بياكل خير شعبه ✌👊 #لم_تسقط_بعد ✌👊 \n"," Actual [start] #تسقط_بس يا سيسي يا بشير يا بن سلمان يا بن زايد يا كل حاكم **** ابن **** ***** بياكل خير شعبه ✌👊 #لم_تسقط_بعد ✌👊 [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: لن تكون ارحم من المملكه وأبناءها وحكامها على قطر وأهل قطر يا مرتزق .. الله يأخذك وأشكالك ويفكنا شركم يا عبدة المال يا خونه انتم الفتنه قبحكم الله \n"," Actual [start] لن تكون ارحم من المملكه وأبناءها وحكامها على قطر وأهل قطر يا ***** .. الله يأخذك و****** ويفكنا شركم يا **** المال يا **** انتم ****** ***** الله [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: وأعره عليكم طرابلس و مصراته ماتجيبيش سيرتها ي قواد يا ذيل يا حافر عدي اعبد حفتر و خلونا في حالنا يا كوالات خلوكم في الانسحاب التكتيكي متاعكم ي كولاات ح يتم الدحر لعند بنغازي ويتم تصفية الطحالب يلي زيك كس بعيد يلا يا قواد \n"," Actual [start] و**** عليكم طرابلس و مصراته ماتجيبيش سيرتها ي **** يا *** يا حافر عدي اعبد حفتر و خلونا في حالنا يا كوالات خلوكم في الانسحاب التكتيكي متاعكم ي كولاات ح يتم الدحر لعند بنغازي ويتم تصفية الطحالب يلي زيك ** بعيد يلا يا **** [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: تحالف العدوان مع المرتزقة والخونة والعملاء سيصعقون وبقووووة بقوووووة للأتي القريب جدا يارب عجل بالفرج يا الله يا الله يا قوي يا متين ارينا فيهم عجائب قدرتك بحق محمد واله الاطهار \n"," Actual [start] تحالف العدوان مع ******** و****** و******* سيصعقون وبقووووة بقوووووة للأتي القريب جدا يارب عجل بالفرج يا الله يا الله يا قوي يا متين ارينا فيهم عجائب قدرتك بحق محمد واله الاطهار [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: نفسي اديك بالقفا يا ابن الكلب يا معرص انت ابوك لابو امثالك \n"," Actual [start] نفسي اديك ****** يا ابن ***** يا **** انت ابوك لابو امثالك [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.02941176470588235 \n"," blue 2-gram :  0.17149858514250882 \n"," blue 3-gram :  0.3123287459051282 \n"," blue 4-gram :  0.41412387656655203 \n","\n","Input: يا خائن يا *** ما تسلم حتى \n"," Actual [start] يا **** يا *** ما تسلم حتى [end] \n"," Predicted [start] *** [en]                  \n","blue 1-gram :  0.0588235294117647 \n"," blue 2-gram :  0.24253562503633297 \n"," blue 3-gram :  0.392601410850376 \n"," blue 4-gram :  0.4924790605054523 \n","\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-15f06191ab71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0minput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_inp_texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# print(input_sentence[i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;31m# score = sentence_bleu(i, translated, weights=(1, 0, 0, 0))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-1d9a73c8479f>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_sentence)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_decoded_sentence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtokenized_target_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarg_vectorization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecoded_sentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenized_input_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_target_sentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0msampled_token_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \"\"\"\n\u001b[1;32m    414\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m--> 415\u001b[0;31m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \"\"\"\n\u001b[1;32m    414\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m--> 415\u001b[0;31m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-feb73a0b6af1>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, encoder_outputs, mask)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         attention_output_1 = self.attention_1(\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         )\n\u001b[1;32m     85\u001b[0m         \u001b[0mout_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattention_output_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/multi_head_attention.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, query, value, key, attention_mask, return_attention_scores, training)\u001b[0m\n\u001b[1;32m    503\u001b[0m     attention_output, attention_scores = self._compute_attention(\n\u001b[1;32m    504\u001b[0m         query, key, value, attention_mask, training)\n\u001b[0;32m--> 505\u001b[0;31m     \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_attention_scores\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/einsum_dense.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0mret\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/special_math_ops.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(equation, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;34m-\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0mare\u001b[0m \u001b[0minconsistent\u001b[0m \u001b[0;32mwith\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m   \"\"\"\n\u001b[0;32m--> 751\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_einsum_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/special_math_ops.py\u001b[0m in \u001b[0;36m_einsum_v2\u001b[0;34m(equation, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1178\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mellipsis_label\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0mresolved_equation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolved_equation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mellipsis_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_linalg_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolved_equation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[0;31m# Send fully specified shapes to opt_einsum, since it cannot handle unknown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_linalg_ops.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(inputs, equation, name)\u001b[0m\n\u001b[1;32m   1071\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 1073\u001b[0;31m         _ctx, \"Einsum\", name, inputs, \"equation\", equation)\n\u001b[0m\u001b[1;32m   1074\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"VjVwnT-tljio","executionInfo":{"status":"aborted","timestamp":1630098957594,"user_tz":-180,"elapsed":10,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["# from nltk.translate.bleu_score import sentence_bleu,corpus_bleu\n","\n","# predicted_list = []\n","# test_inp_texts = [pair[0] for pair in test_pairs]\n","# test_targ_texts = [pair[1] for pair in test_pairs]\n","# for i,j in zip(test_inp_texts,test_targ_texts):\n","#     input_sentence = test_inp_texts\n","#     translated = decode_sequence(i)\n","#     predicted = list(translated.split(\",\"))\n","#     print(\"Input:\",i,\"\\n Actual\",j,\"\\n Predicted\",translated)\n","#     # print(translated)\n","  \n","#     # print(predicted)\n","#     score = sentence_bleu(i, translated, weights=(1, 0, 0, 0))\n","#     print(\"blue score : \",score,\"\\n\\n\")\n","   \n","#     predicted_list.append(predicted)\n","    \n","\n","# res = \"\\n\\n\\n\".join(\"Input: {} \\nActual: {} \\nPredicted: {}\".format(x, y,z) for x, y, z in zip(test_inp_texts[:5], test_targ_texts[:5], predicted_list[:5]))\n","# print(res)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nxbDXWsZUVJf","executionInfo":{"status":"aborted","timestamp":1630098957596,"user_tz":-180,"elapsed":11,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["# bleu_dic = {}\n","# bleu_dic['1-grams'] = sentence_bleu(test_targ_texts, predicted_list, weights=(1.0, 0, 0, 0))\n","# bleu_dic['1-2-grams'] = corpus_bleu(test_targ_texts, predicted_list, weights=(0.5, 0.5, 0, 0))\n","# bleu_dic['1-3-grams'] = corpus_bleu(test_targ_texts, predicted_list, weights=(0.3, 0.3, 0.3, 0))\n","# bleu_dic['1-4-grams'] = corpus_bleu(test_targ_texts, predicted_list, weights=(0.25, 0.25, 0.25, 0.25))   \n","\n","# print(\" \\n-------------\\n BLUE SCORE : \\n-------------\\n \",bleu_dic, \"\\n\\n\\n-------------\\n\")"],"execution_count":null,"outputs":[]}]}
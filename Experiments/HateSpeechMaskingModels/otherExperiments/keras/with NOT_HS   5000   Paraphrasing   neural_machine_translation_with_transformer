{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"with NOT_HS | 5000 | Paraphrasing | neural_machine_translation_with_transformer","provenance":[{"file_id":"1AKMMMWo52uqXoLjFRwsPUPaLcYxYuuWR","timestamp":1630059224425},{"file_id":"16_nfzINfxjl0wXe7OmHg3R1YLoR-mcnp","timestamp":1629471209050},{"file_id":"1AKVcu4C08_OWa07Ghl84q6WmpijlrClk","timestamp":1629307981327},{"file_id":"1u2x4I-WagUER4QJNd4tt0bkz8ilfnDly","timestamp":1628998157816},{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/nlp/ipynb/neural_machine_translation_with_transformer.ipynb","timestamp":1628672106359}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"cells":[{"cell_type":"markdown","metadata":{"id":"OoI1jDpua-wl"},"source":["# English-to-Spanish translation with a sequence-to-sequence Transformer\n","\n","**Author:** [fchollet](https://twitter.com/fchollet)<br>\n","**Date created:** 2021/05/26<br>\n","**Last modified:** 2021/05/26<br>\n","**Description:** Implementing a sequence-to-sequene Transformer and training it on a machine translation task."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bTHMBYKRbi1T","executionInfo":{"status":"ok","timestamp":1630758971582,"user_tz":-180,"elapsed":28492,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}},"outputId":"3bfb025c-81f2-4592-fb8c-4007b12be3d6"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"QC6rAuTna-wp"},"source":["## Introduction\n","\n","In this example, we'll build a sequence-to-sequence Transformer model, which\n","we'll train on an English-to-Spanish machine translation task.\n","\n","You'll learn how to:\n","\n","- Vectorize text using the Keras `TextVectorization` layer.\n","- Implement a `TransformerEncoder` layer, a `TransformerDecoder` layer,\n","and a `PositionalEmbedding` layer.\n","- Prepare data for training a sequence-to-sequence model.\n","- Use the trained model to generate translations of never-seen-before\n","input sentences (sequence-to-sequence inference).\n","\n","The code featured here is adapted from the book\n","[Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition)\n","(chapter 11: Deep learning for text).\n","The present example is fairly barebones, so for detailed explanations of\n","how each building block works, as well as the theory behind Transformers,\n","I recommend reading the book."]},{"cell_type":"markdown","metadata":{"id":"7il3-WZPa-wr"},"source":["## Setup"]},{"cell_type":"code","metadata":{"id":"ctfudCVLVpA6"},"source":["# !pip install --upgrade tensorflow\n","# !pip install --upgrade tensorflow-gpu"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mxIpmPJfa-ws","executionInfo":{"status":"ok","timestamp":1630759052901,"user_tz":-180,"elapsed":2356,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["import pathlib\n","import random\n","import string\n","import re\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6WkEVy-Za-wt"},"source":["## Downloading the data\n","\n","We'll be working with an English-to-Spanish translation dataset\n","provided by [Anki](https://www.manythings.org/anki/). Let's download it:"]},{"cell_type":"code","metadata":{"id":"W8FYDJLEa-wu","executionInfo":{"status":"ok","timestamp":1630759052902,"user_tz":-180,"elapsed":8,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["# text_file = keras.utils.get_file(\n","#     fname=\"spa-eng.zip\",\n","#     origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n","#     extract=True,\n","# )\n","# text_file = pathlib.Path('/content/drive/MyDrive/MasterThesis/paraphrasing/ParapgrasingMask/Parapgrasing - Masking - maskWithWords.tsv')\n","text_file = pathlib.Path('/content/drive/MyDrive/MasterThesis/paraphrasing/NOTHS_Parapgrasing - Masking - MaskWithoutWords.tsv')\n","# text_file = pathlib.Path('/content/drive/MyDrive/MasterThesis/paraphrasing/ParapgrasingMask/ParapgrasingMasking.tsv')"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3GIGj5-Qa-wv"},"source":["## Parsing the data\n","\n","Each line contains an English sentence and its corresponding Spanish sentence.\n","The English sentence is the *source sequence* and Spanish one is the *target sequence*.\n","We prepend the token `\"[start]\"` and we append the token `\"[end]\"` to the Spanish sentence."]},{"cell_type":"code","metadata":{"id":"TemcqrMsa-ww","executionInfo":{"status":"ok","timestamp":1630759060106,"user_tz":-180,"elapsed":397,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["with open(text_file) as f:\n","    lines = f.read().split(\"\\n\")[:-1]\n","text_pairs = []\n","for line in lines:\n","    inp, targ = line.split(\"\\t\")\n","    targ = \"[start] \" + targ + \" [end]\"\n","    text_pairs.append((inp, targ))"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uVT6PBMga-wx"},"source":["Here's what our sentence pairs look like:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RVHW_4Gva-wy","executionInfo":{"status":"ok","timestamp":1630759061113,"user_tz":-180,"elapsed":5,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}},"outputId":"78d1bf37-3636-4920-cab1-2bec70cf8df7"},"source":["for _ in range(5):\n","    print(random.choice(text_pairs))"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["('- Ø¥Ù†ØªÙŠ Ù†ÙƒØ¯ÙŠØ© ! = ÙŠØ§ Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ø¨Ø­Ø± Ø§Ù„ÙØ±Ø­ ÙŠØ§ ØªØ±Ø¹Ø© Ø§Ù„Ù…Ø±Ø­ ÙŠØ§ Ù‡Ø±Ù…ÙˆÙ† Ø§Ù„Ø³Ø¹Ø§Ø¯Ø©..', '[start] - Ø¥Ù†ØªÙŠ Ù†ÙƒØ¯ÙŠØ© ! = ÙŠØ§ Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒ ÙŠØ§ Ø¨Ø­Ø± Ø§Ù„ÙØ±Ø­ ÙŠØ§ ****Ø§Ù„Ù…Ø±Ø­ ÙŠØ§ Ù‡Ø±Ù…ÙˆÙ† Ø§Ù„Ø³Ø¹Ø§Ø¯Ø©.. [end]')\n","('Ù…Ø³ÙƒØªÙŠÙ‡ Ù…Ù† Ø§Ù„Ø§ÙŠØ¯ Ø§Ù„Ù„ÙŠ Ø¨ØªÙˆØ¬Ø¹Ù‡ ğŸ˜‚ğŸ˜‚ ÙŠØ§ Ø§Ø®ÙˆÙ†Ø¬ÙŠ ÙŠØ§ Ù…Ù†ØµÙˆØ± ÙŠØ§ Ù…Ø³ØªØªØ±Ùƒ ğŸ˜', '[start] Ù…Ø³ÙƒØªÙŠÙ‡ Ù…Ù† Ø§Ù„Ø§ÙŠØ¯ Ø§Ù„Ù„ÙŠ Ø¨ØªÙˆØ¬Ø¹Ù‡ ğŸ˜‚ğŸ˜‚ ÙŠØ§ ******ÙŠØ§ Ù…Ù†ØµÙˆØ± ÙŠØ§ Ù…Ø³ØªØªØ±Ùƒ ğŸ˜ [end]')\n","('ÙƒÙ„Ø¨ ÙŠØ§ Ù…Ø¬Ù†Ø³ ÙŠØ§ Ù…Ø´Ø±Ø¯ Ø®Ø±Ø§Ø² Ù†Ø¹ÙˆÙ„ Ø§Ø­Ø°ÙŠÙ‡ Ø¬Ø§Ø¨Ø± Ø²Ø§Ø¯Ù‡ Ø´Ø§Ù‡ Ø§Ù„Ø¥ÙŠØ±Ø§Ù†ÙŠ ØªÙƒØ°Ø¨ Ø§ÙƒØ«Ø± Ù…Ù† ØªÙ†ÙØ³Ùƒ', '[start] ***ÙŠØ§ Ù…Ø¬Ù†Ø³ ÙŠØ§ ****Ø®Ø±Ø§Ø² Ù†Ø¹ÙˆÙ„ Ø§Ø­Ø°ÙŠÙ‡ Ø¬Ø§Ø¨Ø± Ø²Ø§Ø¯Ù‡ Ø´Ø§Ù‡ Ø§Ù„Ø¥ÙŠØ±Ø§Ù†ÙŠ ØªÙƒØ°Ø¨ Ø§ÙƒØ«Ø± Ù…Ù† ØªÙ†ÙØ³Ùƒ [end]')\n","('ØªÙ Ø¹Ù„ÙŠÙƒ ÙˆØ¹Ù„Ù‰ Ø§Ù„ÙŠ Ø®Ù„ÙÙˆÙƒ ÙŠØ§ ÙƒÙ„Ø¨ ÙŠØ§ Ø­Ù‚ÙŠØ± ÙŠØ§ Ø­Ù„Ùˆ Ø´ÙˆÙƒØª ØªØ¨Ø·Ù„ Ø¬Ù…Ø§Ù„ ÙŠØ§ Ø­ÙŠÙˆØ§Ù† ğŸ’”Ø§Ù„Ù„Ù‡ ÙŠØ®Ù„ÙŠÙƒ Ùˆ ÙŠØ­ÙØ¶ÙƒğŸ¥´ğŸ’™', '[start] **Ø¹Ù„ÙŠÙƒ ÙˆØ¹Ù„Ù‰ Ø§Ù„ÙŠ Ø®Ù„ÙÙˆÙƒ ÙŠØ§ ***ÙŠØ§ ****ÙŠØ§ Ø­Ù„Ùˆ Ø´ÙˆÙƒØª ØªØ¨Ø·Ù„ Ø¬Ù…Ø§Ù„ ÙŠØ§ *****ğŸ’”Ø§Ù„Ù„Ù‡ ÙŠØ®Ù„ÙŠÙƒ Ùˆ ÙŠØ­ÙØ¶ÙƒğŸ¥´ğŸ’™ [end]')\n","('Ø§Ù„ØµØ¯Ø§Ø±Ø© Ù„Ùƒ Ø¹Ø±Ø´ ÙŠØ§ Ø²Ø¹ÙŠÙ… Ùˆ Ø£Ø¬Ù„Ø³ ÙŠØ§ Ù…Ù„Ùƒ ğŸ’™ÙˆÙ†Ù‚ÙˆÙ„ğŸ‘‡ğŸ»ğŸ˜ğŸ”¥ #ØµØ¯Ø§Ø±Ù‡_Ø¨Ø³', '[start]  [end]')\n"]}]},{"cell_type":"markdown","metadata":{"id":"Z5tRhKiMa-wz"},"source":["Now, let's split the sentence pairs into a training set, a validation set,\n","and a test set."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8mUPXPWYa-wz","executionInfo":{"status":"ok","timestamp":1630759063881,"user_tz":-180,"elapsed":344,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}},"outputId":"fced29d3-2901-427b-9443-728f113df8c0"},"source":["random.shuffle(text_pairs)\n","num_val_samples = int(0.25 * len(text_pairs))\n","num_train_samples = len(text_pairs) -  num_val_samples\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n","# test_pairs = text_pairs[num_train_samples + num_val_samples :]\n","\n","print(f\"{len(text_pairs)} total pairs\")\n","print(f\"{len(train_pairs)} training pairs\")\n","print(f\"{len(val_pairs)} validation pairs\")\n","# print(f\"{len(test_pairs)} test pairs\")"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["2412 total pairs\n","1809 training pairs\n","603 validation pairs\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IfI1L6L2l-bh","executionInfo":{"status":"ok","timestamp":1630079293293,"user_tz":-180,"elapsed":7,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}},"outputId":"1e000630-03d9-4393-b27e-22d55234a68d"},"source":["test_file = pathlib.Path('/content/drive/MyDrive/MasterThesis/paraphrasing/paraphrasingTest.tsv')\n","with open(test_file) as f:\n","    lines = f.read().split(\"\\n\")[:-1]\n","test_pairs = []\n","for line in lines:\n","    inp, targ = line.split(\"\\t\")\n","    targ = \"[start] \" + targ + \" [end]\"\n","    test_pairs.append((inp, targ))\n","\n","for _ in range(5):\n","    print(random.choice(test_pairs))\n","\n","random.shuffle(test_pairs)\n","num_test_samples = len(test_pairs)\n","test_pairs = test_pairs[: num_test_samples]\n","print(f\"{len(test_pairs)} test pairs\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["('ÙŠØ§ Ø³ÙŠØ³ÙŠ ÙŠØ§ Ù…ØºÙˆØ§Ø± ÙŠØ§ Ù‚Ø§Ù‡Ø± Ø¥Ø®ÙˆØ§Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ø£Ø¶Ø±Ø¨ Ø¨Ø¬ÙŠØ´Ùƒ Ø§Ù„Ø¬Ø±Ø§Ø± Ø£Ø¶Ø±Ø¨ ÙŠØ§ Ø³ÙŠØ³ÙŠ ÙƒÙ„ Ø®Ø§ÙŠÙ† ÙˆØºØ¯Ø§Ø± Ù…Ù† Ù‚Ø·Ø± ÙˆØªØ±ÙƒÙŠØ§ ÙˆØ¥ÙŠØ±Ø§Ù† Ø£Ø¹Ø¯Ø§Ø¡ Ø§Ù„Ù„Ù‡ ÙˆØ§Ù„Ø¥Ø³Ù„Ø§Ù… Ø¥Ø¶Ø±Ø§Ø¨ ÙŠØ§ Ø³ÙŠØ³ÙŠ ÙŠØ§ Ù…ØºÙˆØ§Ø±', '[start] ÙŠØ§ Ø³ÙŠØ³ÙŠ ÙŠØ§ Ù…ØºÙˆØ§Ø± ÙŠØ§ **** Ø¥Ø®ÙˆØ§Ù† Ø§Ù„Ø­Ø±Ø§Ù… Ø£Ø¶Ø±Ø¨ Ø¨Ø¬ÙŠØ´Ùƒ Ø§Ù„Ø¬Ø±Ø§Ø± Ø£Ø¶Ø±Ø¨ ÙŠØ§ Ø³ÙŠØ³ÙŠ ÙƒÙ„ **** Ùˆ**** Ù…Ù† Ù‚Ø·Ø± ÙˆØªØ±ÙƒÙŠØ§ ÙˆØ¥ÙŠØ±Ø§Ù† Ø£Ø¹Ø¯Ø§Ø¡ Ø§Ù„Ù„Ù‡ ÙˆØ§Ù„Ø¥Ø³Ù„Ø§Ù… Ø¥Ø¶Ø±Ø§Ø¨ ÙŠØ§ Ø³ÙŠØ³ÙŠ ÙŠØ§ Ù…ØºÙˆØ§Ø± [end]')\n","('Ø¨ØªØ®Ø§Ù Ù†Ø³ÙˆØ§Ù†Ùƒ ÙŠØ²Ø¹Ù„ÙˆØ§ ÙˆÙ„Ø§ Ø§ÙŠÙ‡ ğŸ˜‚ Ø§Ù‡ ÙŠØ§ Ù‡Ù„ÙÙˆØª ÙŠØ§ Ø¨ØªØ§Ø¹ Ø§Ù„ÙˆØ±Ø¯ ğŸ˜‚', '[start] Ø¨ØªØ®Ø§Ù Ù†Ø³ÙˆØ§Ù†Ùƒ ÙŠØ²Ø¹Ù„ÙˆØ§ ÙˆÙ„Ø§ Ø§ÙŠÙ‡ ğŸ˜‚ Ø§Ù‡ ÙŠØ§ ***** ÙŠØ§ Ø¨ØªØ§Ø¹ Ø§Ù„ÙˆØ±Ø¯ ğŸ˜‚ [end]')\n","('Ø§Ø³Ù…Ø¹ÙˆØ§ ÙŠØ§ Ø®ÙˆÙ†Ø© Ø§Ø³Ù…Ø¹ÙˆØ§ ÙŠØ§ Ø¹Ù…Ù„Ø§Ø¡ Ø§Ù„ÙˆØ·Ù† Ø§Ù‡Ù… Ø´ÙŠ ÙÙ‰ Ø§Ù„Ø¯Ù†ÙŠØ§ ğŸ‡ªğŸ‡¬ğŸ‡ªğŸ‡¬ğŸ‡ªğŸ‡¬ğŸ‡ªğŸ‡¬', '[start] Ø§Ø³Ù…Ø¹ÙˆØ§ ÙŠØ§ **** Ø§Ø³Ù…Ø¹ÙˆØ§ ÙŠØ§ ***** Ø§Ù„ÙˆØ·Ù† Ø§Ù‡Ù… Ø´ÙŠ ÙÙ‰ Ø§Ù„Ø¯Ù†ÙŠØ§ ğŸ‡ªğŸ‡¬ğŸ‡ªğŸ‡¬ğŸ‡ªğŸ‡¬ğŸ‡ªğŸ‡¬ [end]')\n","('ÙÙŠ Ø­Ø¯ ÙƒØ¯Ø§ ÙÙŠ Ø¨Ø§Ù„ÙŠ Ù†ÙØ³ÙŠ Ø£Ø±ÙˆØ­ Ø§Ø¯ÙŠÙ„Ù‡ Ø¨Ø§Ù„Ù‚Ù„Ù… ÙˆØ£Ù‚ÙˆÙ„Ù‡ Ø¥ÙŠÙ‡ ÙŠØ§ Ø¬Ø¨Ù„Ø© ÙŠØ§ Ø¹Ø¯ÙŠÙ… Ø§Ù„Ø§Ø­Ø³Ø§Ø³ ÙŠØ§ Ù…Ù‡Ø²Ø£ ÙˆØ¨Ø¹Ø¯Ù‡Ø§ Ù‡Ø±ØªØ§Ø­ Ø¹Ø§Ø¯ÙŠğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚', '[start] ÙÙŠ Ø­Ø¯ ÙƒØ¯Ø§ ÙÙŠ Ø¨Ø§Ù„ÙŠ Ù†ÙØ³ÙŠ Ø£Ø±ÙˆØ­ Ø§Ø¯ÙŠÙ„Ù‡ ****** ÙˆØ£Ù‚ÙˆÙ„Ù‡ Ø¥ÙŠÙ‡ ÙŠØ§ **** ÙŠØ§ **** Ø§Ù„Ø§Ø­Ø³Ø§Ø³ ÙŠØ§ **** ÙˆØ¨Ø¹Ø¯Ù‡Ø§ Ù‡Ø±ØªØ§Ø­ Ø¹Ø§Ø¯ÙŠğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ [end]')\n","('Ø¯Ù‡ Ù…Ù‚Ø·Ø¹ Ù„Ø£ØºÙ†ÙŠØ© Ø§Ù„ØªØ±Ø§Ø³ Ø§Ù‡Ù„Ø§ÙˆÙŠ Ø§Ù„Ù„Ù‰ ØºÙ†ÙˆÙ‡Ø§ Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø°Ø¨Ø­Ù‡ : Ø§Ù„ÙƒÙˆØ±Ø© Ù„Ù…Ø§ Ø¬ÙŠÙ†Ø§ ÙƒØ§Ù†Øª ÙƒØ¯Ø¨ ÙˆÙƒØ§Ù†Øª Ø®Ø¯Ø§Ø¹ ÙƒØ§Ù†Øª Ù…ØºÙŠØ¨Ù‡ Ù„Ù„Ø¹Ù‚ÙˆÙ„ ÙƒØ§Ù†Øª Ù„Ù„Ø³Ù„Ø·Ø© Ù‚Ù†Ø§Ø¹ Ø¨ÙŠØ­Ø§ÙˆÙ„ÙˆØ§ ÙŠÙƒÙ…Ù„ÙˆÙ‡Ø§ ÙˆØªØ¨Ù‚Ù‰ Ù‡Ù… Ù„Ù„Ø¨Ù„Ø§Ø¯ . Ø§Ù‡ ÙŠØ§ Ù…Ø¬Ù„Ø³ ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„Ø­Ø±Ø§Ù… . ÙŠØ¹Ù†Ù‰ Ø¹Ø§Ø±ÙÙŠÙ† Ø§Ù†Ù‡Ø§ Ø§Ù„Ù‡Ø§Ø¡ ÙˆÙ…Ø¹ Ø°Ø§Ù„Ùƒ Ø­', '[start] Ø¯Ù‡ Ù…Ù‚Ø·Ø¹ Ù„Ø£ØºÙ†ÙŠØ© Ø§Ù„ØªØ±Ø§Ø³ Ø§Ù‡Ù„Ø§ÙˆÙŠ Ø§Ù„Ù„Ù‰ ØºÙ†ÙˆÙ‡Ø§ Ø¨Ø¹Ø¯ ******* : Ø§Ù„ÙƒÙˆØ±Ø© Ù„Ù…Ø§ Ø¬ÙŠÙ†Ø§ ÙƒØ§Ù†Øª *** ÙˆÙƒØ§Ù†Øª **** ÙƒØ§Ù†Øª Ù…ØºÙŠØ¨Ù‡ Ù„Ù„Ø¹Ù‚ÙˆÙ„ ÙƒØ§Ù†Øª Ù„Ù„Ø³Ù„Ø·Ø© Ù‚Ù†Ø§Ø¹ Ø¨ÙŠØ­Ø§ÙˆÙ„ÙˆØ§ ÙŠÙƒÙ…Ù„ÙˆÙ‡Ø§ ÙˆØªØ¨Ù‚Ù‰ Ù‡Ù… Ù„Ù„Ø¨Ù„Ø§Ø¯ . Ø§Ù‡ ÙŠØ§ Ù…Ø¬Ù„Ø³ ÙŠØ§ Ø§Ø¨Ù† ****** . ÙŠØ¹Ù†Ù‰ Ø¹Ø§Ø±ÙÙŠÙ† Ø§Ù†Ù‡Ø§ Ø§Ù„Ù‡Ø§Ø¡ ÙˆÙ…Ø¹ Ø°Ø§Ù„Ùƒ Ø­ [end]')\n","401 test pairs\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1CpqzqNaa-w0"},"source":["## Vectorizing the text data\n","\n","We'll use two instances of the `TextVectorization` layer to vectorize the text\n","data (one for English and one for Spanish),\n","that is to say, to turn the original strings into integer sequences\n","where each integer represents the index of a word in a vocabulary.\n","\n","The English layer will use the default string standardization (strip punctuation characters)\n","and splitting scheme (split on whitespace), while\n","the Spanish layer will use a custom standardization, where we add the character\n","`\"Â¿\"` to the set of punctuation characters to be stripped.\n","\n","Note: in a production-grade machine translation model, I would not recommend\n","stripping the punctuation characters in either language. Instead, I would recommend turning\n","each punctuation character into its own token,\n","which you could achieve by providing a custom `split` function to the `TextVectorization` layer."]},{"cell_type":"code","metadata":{"id":"CPoQesZea-w0"},"source":["# strip_chars = string.punctuation + \"Â¿\"\n","strip_chars = \"[a-zA-Z]|\\d+|[Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©]|[.#ØŒ<>@,\\\\-_â€â€œÙªÙÙ‹]\"\n","\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n","\n","vocab_size = 7000\n","sequence_length = 20\n","batch_size = 64\n","\n","\n","def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)\n","    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n","\n","\n","inp_vectorization = TextVectorization(\n","    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",")\n","targ_vectorization = TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length + 1,\n","    standardize=custom_standardization,\n",")\n","train_eng_texts = [pair[0] for pair in train_pairs]\n","train_spa_texts = [pair[1] for pair in train_pairs]\n","inp_vectorization.adapt(train_eng_texts)\n","targ_vectorization.adapt(train_spa_texts)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8enIADXMa-w1"},"source":["Next, we'll format our datasets.\n","\n","At each training step, the model will seek to predict target words N+1 (and beyond)\n","using the source sentence and the target words 0 to N.\n","\n","As such, the training dataset will yield a tuple `(inputs, targets)`, where:\n","\n","- `inputs` is a dictionary with the keys `encoder_inputs` and `decoder_inputs`.\n","`encoder_inputs` is the vectorized source sentence and `encoder_inputs` is the target sentence \"so far\",\n","that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target sentence.\n","- `target` is the target sentence offset by one step:\n","it provides the next words in the target sentence -- what the model will try to predict."]},{"cell_type":"code","metadata":{"id":"dKmCVA5Ka-w2"},"source":["\n","def format_dataset(inp, targ):\n","    inp = inp_vectorization(inp)\n","    targ = targ_vectorization(targ)\n","    return ({\"encoder_inputs\": inp, \"decoder_inputs\": targ[:, :-1],}, targ[:, 1:])\n","\n","\n","def make_dataset(pairs):\n","    inp_texts, targ_texts = zip(*pairs)\n","    inp_texts = list(inp_texts)\n","    targ_texts = list(targ_texts)\n","    dataset = tf.data.Dataset.from_tensor_slices((inp_texts, targ_texts))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset)\n","    return dataset.shuffle(248).prefetch(16).cache()\n","\n","\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U6ZQPBz4a-w3"},"source":["Let's take a quick look at the sequence shapes\n","(we have batches of 64 pairs, and all sequences are 20 steps long):"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PkVSFSxBa-w3","executionInfo":{"status":"ok","timestamp":1630079295221,"user_tz":-180,"elapsed":1931,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}},"outputId":"2a75eb6a-e2bb-45fb-907a-6d7ab4e7f64d"},"source":["for inputs, targets in train_ds.take(1):\n","    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n","    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n","    print(f\"targets.shape: {targets.shape}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["inputs[\"encoder_inputs\"].shape: (64, 20)\n","inputs[\"decoder_inputs\"].shape: (64, 20)\n","targets.shape: (64, 20)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GQ6z9fVea-w3"},"source":["## Building the model\n","\n","Our sequence-to-sequence Transformer consists of a `TransformerEncoder`\n","and a `TransformerDecoder` chained together. To make the model aware of word order,\n","we also use a `PositionalEmbedding` layer.\n","\n","The source sequence will be pass to the `TransformerEncoder`,\n","which will produce a new representation of it.\n","This new representation will then be passed\n","to the `TransformerDecoder`, together with the target sequence so far (target words 0 to N).\n","The `TransformerDecoder` will then seek to predict the next words in the target sequence (N+1 and beyond).\n","\n","A key detail that makes this possible is causal masking\n","(see method `get_causal_attention_mask()` on the `TransformerDecoder`).\n","The `TransformerDecoder` sees the entire sequences at once, and thus we must make\n","sure that it only uses information from target tokens 0 to N when predicting token N+1\n","(otherwise, it could use information from the future, which would\n","result in a model that cannot be used at inference time)."]},{"cell_type":"code","metadata":{"id":"3jHRvQaHa-w3"},"source":["\n","class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super(TransformerEncoder, self).__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.dropout = tf.keras.layers.Dropout(0.5)\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n","        attention_output = self.attention(\n","            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n","        )\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","\n","\n","class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n","        super(PositionalEmbedding, self).__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=vocab_size, output_dim=embed_dim\n","        )\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=embed_dim\n","        )\n","        self.sequence_length = sequence_length\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","\n","\n","class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n","        super(TransformerDecoder, self).__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.latent_dim = latent_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.dropout = tf.keras.layers.Dropout(0.5)\n","\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n","            padding_mask = tf.minimum(padding_mask, causal_mask)\n","\n","        attention_output_1 = self.attention_1(\n","            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n","        )\n","        out_1 = self.layernorm_1(inputs + attention_output_1)\n","\n","        attention_output_2 = self.attention_2(\n","            query=out_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","        )\n","        out_2 = self.layernorm_2(out_1 + attention_output_2)\n","\n","        proj_output = self.dense_proj(out_2)\n","        return self.layernorm_3(out_2 + proj_output)\n","\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n","            axis=0,\n","        )\n","        return tf.tile(mask, mult)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WCD0jPjqa-w4"},"source":["Next, we assemble the end-to-end model."]},{"cell_type":"code","metadata":{"id":"PaxOj5PWa-w5"},"source":["embed_dim = 256\n","latent_dim = 2048\n","num_heads = 8\n","\n","encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n","encoder = keras.Model(encoder_inputs, encoder_outputs)\n","\n","decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n","encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","# x = layers.Dropout(0.5)(x)\n","\n","x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n","\n","x = layers.Dropout(0.5)(x)\n","decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","# x = layers.Dropout(0.2)(x)\n","\n","decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n","\n","decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n","transformer = keras.Model(\n","    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9dFxoe3ma-w5"},"source":["## Training our model\n","\n","We'll use accuracy as a quick way to monitor training progress on the validation data.\n","Note that machine translation typically uses BLEU scores as well as other metrics, rather than accuracy.\n","\n","Here we only train for 1 epoch, but to get the model to actually converge\n","you should train for at least 30 epochs."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8tGD8iZLa-w6","executionInfo":{"status":"ok","timestamp":1630079613733,"user_tz":-180,"elapsed":234245,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}},"outputId":"e4993020-c951-4353-dd2a-367196309ef8"},"source":["epochs = 30  # This should be at least 30 for convergence\n","my_callbacks = [\n","    tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=2),\n","    # tf.keras.callbacks.ReduceLROnPlateau(\n","    #   monitor='val_loss', factor=0.1, patience=2,\n","    #   min_lr=0,\n","    # )\n","]\n","\n","transformer.summary()\n","transformer.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",")\n","\n","history  = transformer.fit(train_ds, epochs=epochs, validation_data=val_ds,callbacks=my_callbacks)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"transformer\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","encoder_inputs (InputLayer)     [(None, None)]       0                                            \n","__________________________________________________________________________________________________\n","positional_embedding_8 (Positio (None, None, 256)    1797120     encoder_inputs[0][0]             \n","__________________________________________________________________________________________________\n","decoder_inputs (InputLayer)     [(None, None)]       0                                            \n","__________________________________________________________________________________________________\n","transformer_encoder_4 (Transfor (None, None, 256)    3155456     positional_embedding_8[0][0]     \n","__________________________________________________________________________________________________\n","model_9 (Functional)            (None, None, 7000)   8855640     decoder_inputs[0][0]             \n","                                                                 transformer_encoder_4[0][0]      \n","==================================================================================================\n","Total params: 13,808,216\n","Trainable params: 13,808,216\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Epoch 1/30\n","29/29 [==============================] - 64s 2s/step - loss: 3.7993 - accuracy: 0.2175 - val_loss: 2.4281 - val_accuracy: 0.4589\n","Epoch 2/30\n","29/29 [==============================] - 59s 2s/step - loss: 3.1942 - accuracy: 0.2927 - val_loss: 2.2603 - val_accuracy: 0.4722\n","Epoch 3/30\n","29/29 [==============================] - 60s 2s/step - loss: 3.0422 - accuracy: 0.3053 - val_loss: 2.2729 - val_accuracy: 0.4553\n","Epoch 4/30\n","29/29 [==============================] - 59s 2s/step - loss: 2.9098 - accuracy: 0.3155 - val_loss: 2.3428 - val_accuracy: 0.4586\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"id":"AoxqTnCF_2tK","executionInfo":{"status":"ok","timestamp":1630080461563,"user_tz":-180,"elapsed":2352,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}},"outputId":"e6455207-953f-444e-f66d-c07b2c808f49"},"source":["import matplotlib.pyplot as plt\n","\n","s, (at, al) = plt.subplots(2,1)\n","at.plot(history.history['accuracy'], c= 'b')\n","at.plot(history.history['val_accuracy'], c='r')\n","at.set_title('model accuracy')\n","at.set_ylabel('accuracy')\n","at.set_xlabel('epoch')\n","at.legend(['train', 'val'], loc='upper left')\n","\n","al.plot(history.history['loss'], c='m')\n","al.plot(history.history['val_loss'], c='c')\n","al.set_title('model loss')\n","al.set_ylabel('loss')\n","al.set_xlabel('epoch')\n","al.legend(['train', 'val'], loc = 'upper left')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7f863358ab90>"]},"metadata":{},"execution_count":91},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"SL-vwLH1a-w6"},"source":["## Decoding test sentences\n","\n","Finally, let's demonstrate how to translate brand new English sentences.\n","We simply feed into the model the vectorized English sentence\n","as well as the target token `\"[start]\"`, then we repeatedly generated the next token, until\n","we hit the token `\"[end]\"`."]},{"cell_type":"code","metadata":{"id":"okeujBqTa-w6"},"source":["spa_vocab = targ_vectorization.get_vocabulary()\n","spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n","max_decoded_sentence_length = 20\n","\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = inp_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = targ_vectorization([decoded_sentence])[:, :-1]\n","        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n","\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = spa_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","\n","        if sampled_token == \"[end]\":\n","            break\n","    return decoded_sentence\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qZlMgW0dnxno"},"source":["# from nltk.translate.bleu_score import corpus_bleu\n","# from nltk.translate.bleu_score import sentence_bleu\n","# predicted_list = []\n","# test_inp_texts = [pair[0] for pair in test_pairs]\n","# test_targ_texts = [pair[1] for pair in test_pairs]\n","\n","# score_list = []\n","# from nltk.translate.bleu_score import SmoothingFunction\n","# # print(len(test_inp_texts))\n","# # print(len(test_targ_texts))\n","# # def bleu_score():\n","\n","# for i,j in zip(test_inp_texts,test_targ_texts):\n","\n","#   input_sentence = test_inp_texts\n","#     # print(input_sentence[i])\n","#   translated = decode_sequence(i)\n","#   predicted = list(translated.split(\",\"))\n","#   score = sentence_bleu(i, translated, weights=(1, 0, 0, 0))\n","#   score_list.append(score)\n","#   predicted_list.append(predicted)\n","#   print(\"Input:\",i,\"\\n Actual\",j,\"\\n Predicted\",translated)\n","#   score = sentence_bleu(i, translated, weights=(1, 0, 0, 0))\n","#   print(\"blue score : \",score,\"\\n\\n\")\n","# avg = sum(score_list) / len(score_list)\n","# print(\"Average of the list =\", round(avg, 2))\n","\n","#   # return bleu_dic\n","\n","\n","# # bleu_score()\n","# # bleu_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hiKXV65HxspA"},"source":["from nltk.translate.bleu_score import corpus_bleu\n","from nltk.translate.bleu_score import sentence_bleu\n","predicted_list = []\n","test_inp_texts = [pair[0] for pair in test_pairs]\n","test_targ_texts = [pair[1] for pair in test_pairs]\n","\n","score_list = []\n","score_list_2 = []\n","score_list_3 = []\n","score_list_4 = []\n","from nltk.translate.bleu_score import SmoothingFunction\n","# print(len(test_inp_texts))\n","# print(len(test_targ_texts))\n","# def bleu_score():\n","\n","for i,j in zip(test_inp_texts,test_targ_texts):\n","\n","  input_sentence = test_inp_texts\n","    # print(input_sentence[i])\n","  translated = decode_sequence(i)\n","  predicted = list(translated.split(\",\"))\n","  # score = sentence_bleu(i, translated, weights=(1, 0, 0, 0))\n","  score_1 = sentence_bleu(i, translated, weights=(1, 0, 0, 0))\n","  score_2 = sentence_bleu(i, translated,weights=(0.5, 0.5, 0, 0))\n","  score_3 = sentence_bleu(i, translated, weights=(0.33, 0.33, 0.33, 0))\n","  score_4 = sentence_bleu(i, translated, weights=(0.25, 0.25, 0.25, 0.25))\n","  score_list.append(score_1)\n","  score_list_2.append(score_2)\n","  score_list_3.append(score_3)\n","  score_list_4.append(score_4)\n","  predicted_list.append(predicted)\n","  print(\"Input:\",i,\"\\n Actual\",j,\"\\n Predicted\",translated)\n","  # score_1 = sentence_bleu(i, translated, weights=(1, 0, 0, 0))\n","  # score_2 = sentence_bleu(i, translated,weights=(0.5, 0.5, 0, 0))\n","  # score_3 = sentence_bleu(i, translated, weights=(0.33, 0.33, 0.33, 0))\n","  # score_4 = sentence_bleu(i, translated, weights=(0.25, 0.25, 0.25, 0.25))\n","  print(\"blue 1-gram : \",score_1,\"\\n\",\"blue 2-gram : \",score_2,\"\\n\",\"blue 3-gram : \",score_3,\"\\n\",\"blue 4-gram : \",score_4,\"\\n\")\n","avg = sum(score_list) / len(score_list)\n","avg_2 = sum(score_list_2) / len(score_list_2)\n","avg_3 = sum(score_list_3) / len(score_list_3)\n","avg_4 = sum(score_list_4) / len(score_list_4)\n","print(\"Average of the list 1-gram=\", round(avg, 2),\"Average of the list 2-gram=\", round(avg_2, 2),\"Average of the list 3-gram=\", \n","      round(avg_3, 2),\"Average of the list 4-gram=\", round(avg_4, 2))\n","\n","  # return bleu_dic\n","\n","\n","# bleu_score()\n","# bleu_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VjVwnT-tljio"},"source":["# from nltk.translate.bleu_score import sentence_bleu,corpus_bleu\n","\n","# predicted_list = []\n","# test_inp_texts = [pair[0] for pair in test_pairs]\n","# test_targ_texts = [pair[1] for pair in test_pairs]\n","# for i,j in zip(test_inp_texts,test_targ_texts):\n","#     input_sentence = test_inp_texts\n","#     translated = decode_sequence(i)\n","#     predicted = list(translated.split(\",\"))\n","#     print(\"Input:\",i,\"\\n Actual\",j,\"\\n Predicted\",translated)\n","#     # print(translated)\n","  \n","#     # print(predicted)\n","#     score = sentence_bleu(i, translated, weights=(1, 0, 0, 0))\n","#     print(\"blue score : \",score,\"\\n\\n\")\n","   \n","#     predicted_list.append(predicted)\n","    \n","\n","# res = \"\\n\\n\\n\".join(\"Input: {} \\nActual: {} \\nPredicted: {}\".format(x, y,z) for x, y, z in zip(test_inp_texts[:5], test_targ_texts[:5], predicted_list[:5]))\n","# print(res)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nxbDXWsZUVJf"},"source":["# bleu_dic = {}\n","# bleu_dic['1-grams'] = sentence_bleu(test_targ_texts, predicted_list, weights=(1.0, 0, 0, 0))\n","# bleu_dic['1-2-grams'] = corpus_bleu(test_targ_texts, predicted_list, weights=(0.5, 0.5, 0, 0))\n","# bleu_dic['1-3-grams'] = corpus_bleu(test_targ_texts, predicted_list, weights=(0.3, 0.3, 0.3, 0))\n","# bleu_dic['1-4-grams'] = corpus_bleu(test_targ_texts, predicted_list, weights=(0.25, 0.25, 0.25, 0.25))   \n","\n","# print(\" \\n-------------\\n BLUE SCORE : \\n-------------\\n \",bleu_dic, \"\\n\\n\\n-------------\\n\")"],"execution_count":null,"outputs":[]}]}
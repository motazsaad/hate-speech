{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of with clean  3183| L2  | 6000 Paraphrasing - with words | neural_machine_translation_with_transformer","provenance":[{"file_id":"1lnhibBJXz0sp_pqXT6CMDSIrxgbnaMeM","timestamp":1630831688929},{"file_id":"1yYsZ1FGHNePqXTOvs7IIoGuJip-s0jmv","timestamp":1630394815102},{"file_id":"1J82OaBRQAcvxs6VoEAIaDr_8WU38thfV","timestamp":1630210935664},{"file_id":"1u2x4I-WagUER4QJNd4tt0bkz8ilfnDly","timestamp":1629039480801},{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/nlp/ipynb/neural_machine_translation_with_transformer.ipynb","timestamp":1628672106359}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"cells":[{"cell_type":"markdown","metadata":{"id":"OoI1jDpua-wl"},"source":["# Paraphrasing | translation with a sequence-to-sequence Transformer\n","\n","**Author:** [fchollet](https://twitter.com/fchollet)<br>\n","**Date created:** 2021/05/26<br>\n","**Last modified:** 2021/05/26<br>\n","**Description:** Implementing a sequence-to-sequene Transformer and training it on a machine translation task."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":556},"id":"bTHMBYKRbi1T","executionInfo":{"status":"error","timestamp":1631690355405,"user_tz":-180,"elapsed":1833,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}},"outputId":"2ff5bf2c-5db6-4f8d-be7e-7ab4ab3d486f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    618\u001b[0m         \"\"\"\n\u001b[0;32m--> 619\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    106\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       use_metadata_server=use_metadata_server)\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server, ephemeral)\u001b[0m\n\u001b[1;32m    285\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dfs-auth-dance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m           \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"QC6rAuTna-wp"},"source":["## Introduction\n","\n","In this example, we'll build a sequence-to-sequence Transformer model, which\n","we'll train on an English-to-Spanish machine translation task.\n","\n","You'll learn how to:\n","\n","- Vectorize text using the Keras `TextVectorization` layer.\n","- Implement a `TransformerEncoder` layer, a `TransformerDecoder` layer,\n","and a `PositionalEmbedding` layer.\n","- Prepare data for training a sequence-to-sequence model.\n","- Use the trained model to generate translations of never-seen-before\n","input sentences (sequence-to-sequence inference).\n","\n","The code featured here is adapted from the book\n","[Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition)\n","(chapter 11: Deep learning for text).\n","The present example is fairly barebones, so for detailed explanations of\n","how each building block works, as well as the theory behind Transformers,\n","I recommend reading the book."]},{"cell_type":"markdown","metadata":{"id":"7il3-WZPa-wr"},"source":["## Setup"]},{"cell_type":"code","metadata":{"id":"ANw0AJ41G-xR","executionInfo":{"status":"aborted","timestamp":1631690351477,"user_tz":-180,"elapsed":15,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["# !pip install --upgrade tensorflow\n","# !pip install --upgrade tensorflow-gpu"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mxIpmPJfa-ws","executionInfo":{"status":"aborted","timestamp":1631690351478,"user_tz":-180,"elapsed":16,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["import pathlib\n","import random\n","import string\n","import re\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6WkEVy-Za-wt"},"source":["## Downloading the data\n","\n","We'll be working with an English-to-Spanish translation dataset\n","provided by [Anki](https://www.manythings.org/anki/). Let's download it:"]},{"cell_type":"code","metadata":{"id":"W8FYDJLEa-wu","executionInfo":{"status":"aborted","timestamp":1631690351479,"user_tz":-180,"elapsed":16,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["# text_file = keras.utils.get_file(\n","#     fname=\"spa-eng.zip\",\n","#     origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n","#     extract=True,\n","# )\n","text_file = pathlib.Path('/content/drive/MyDrive/MasterThesis/paraphrasing/ParapgrasingMask/ParaphrasingHSWithNotHS.tsv')\n","\n","# text_file = pathlib.Path('/content/drive/MyDrive/MasterThesis/paraphrasing/ParapgrasingMask/ParapgrasingMasking.tsv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3GIGj5-Qa-wv"},"source":["## Parsing the data\n","\n","Each line contains an English sentence and its corresponding Spanish sentence.\n","The English sentence is the *source sequence* and Spanish one is the *target sequence*.\n","We prepend the token `\"[start]\"` and we append the token `\"[end]\"` to the Spanish sentence."]},{"cell_type":"code","metadata":{"id":"TemcqrMsa-ww","executionInfo":{"status":"aborted","timestamp":1631690351480,"user_tz":-180,"elapsed":17,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["with open(text_file) as f:\n","    lines = f.read().split(\"\\n\")[:-1]\n","text_pairs = []\n","for line in lines:\n","    inp, targ = line.split(\"\\t\")\n","    targ = \"[start] \" + targ + \" [end]\"\n","    text_pairs.append((inp, targ))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uVT6PBMga-wx"},"source":["Here's what our sentence pairs look like:"]},{"cell_type":"code","metadata":{"id":"RVHW_4Gva-wy","executionInfo":{"status":"aborted","timestamp":1631690351480,"user_tz":-180,"elapsed":17,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["for _ in range(5):\n","    print(random.choice(text_pairs))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z5tRhKiMa-wz"},"source":["Now, let's split the sentence pairs into a training set, a validation set,\n","and a test set."]},{"cell_type":"code","metadata":{"id":"8mUPXPWYa-wz","executionInfo":{"status":"aborted","timestamp":1631690351481,"user_tz":-180,"elapsed":18,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["random.shuffle(text_pairs)\n","num_val_samples = int(0.25 * len(text_pairs))\n","num_train_samples = len(text_pairs) -  num_val_samples\n","train_pairs = text_pairs[:num_train_samples]\n","val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n","# test_pairs = text_pairs[num_train_samples + num_val_samples :]\n","\n","print(f\"{len(text_pairs)} total pairs\")\n","print(f\"{len(train_pairs)} training pairs\")\n","print(f\"{len(val_pairs)} validation pairs\")\n","# print(f\"{len(test_pairs)} test pairs\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IpQDgKy1I3dm","executionInfo":{"status":"aborted","timestamp":1631690351482,"user_tz":-180,"elapsed":19,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["import re\n","# text_cleaning_re = \"\\d+|[.#،<>@,\\\\-_”“٪ًَ]\"\n","def preprocess(text):\n","  # search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\"\\\\\",'\\n', '\\t','&quot;','?','؟','!','#']\n","  # replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ',' ! ',' ']\n","  #remove tashkeel\n","  # p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n","  # text = re.sub(p_tashkeel,\"\", str(text))\n","  # text = text.replace('وو', 'و')\n","  # text = text.replace('يي', 'ي')\n","  # text = text.replace('اا', 'ا')\n","  # text = re.sub(text_cleaning_re, ' ', str(text)).strip()\n","  #remove longation\n","  text = re.sub(r'(.)\\1+', r'\\1\\1', str(text)) \n","  text = re.sub(\"[إأآا]\", \"ا\", str(text))\n","  text = re.sub(\"ى\", \"ي\", str(text))\n","  # text = re.sub(\"ؤ\", \"ء\", text)\n","  # text = re.sub(\"ئ\", \"ء\", text)\n","  text = re.sub(\"ة\", \"ه\", str(text))\n","\n","  return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0utfkxx8IJtj","executionInfo":{"status":"aborted","timestamp":1631690351482,"user_tz":-180,"elapsed":19,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["train_eng_texts = [pair[0] for pair in train_pairs]\n","from collections import Counter\n","mostFreq = []\n","for words in Counter(\" \".join(train_eng_texts).split()).most_common(5000):\n","  mostFreq.append(words[0])\n","print(mostFreq)\n","print(len(mostFreq))\n","afterpre=map(preprocess, mostFreq)\n","afterpre =list(afterpre)\n","len(afterpre)\n","# badWordList =  pathlib.Path('/content/drive/MyDrive/MasterThesis/paraphrasing/badWords.tsv')\n","# list(badWordList)\n","import pandas as pd\n","badWordList = pd.read_csv('/content/drive/MyDrive/MasterThesis/paraphrasing/badWords.tsv',sep=\"\\t\")\n","badWordList=badWordList.values.tolist()\n","BWL = [''.join(ele) for ele in badWordList]\n","  \n","# printing result\n","print(\"The Bad word list is : \" + str(BWL) )\n","print(\"The length of Bad word list : \" , len(BWL))\n","vocabList = afterpre + BWL\n","\n","print(\"The vocab list is : \" + str(vocabList) )\n","print(\"The length of vocab list : \" , len(vocabList))\n","vocabListWithoutRepeated = list(set(vocabList))\n","print(\"The vocab list is : \" + str(vocabListWithoutRepeated) )\n","print(\"The length of vocab list : \" , len(vocabListWithoutRepeated))\n","vocab_size = 5000\n","sequence_length = 20\n","\n","inp_vectorization = TextVectorization(\n","    max_tokens=None, output_mode=\"int\", output_sequence_length=sequence_length,vocabulary=vocabListWithoutRepeated\n",")\n","print(inp_vectorization.get_vocabulary())\n","print(inp_vectorization.vocabulary_size())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IfI1L6L2l-bh","executionInfo":{"status":"aborted","timestamp":1631690351483,"user_tz":-180,"elapsed":19,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["test_file = pathlib.Path('/content/drive/MyDrive/MasterThesis/paraphrasing/paraphrasingTest.tsv')\n","with open(test_file) as f:\n","    lines = f.read().split(\"\\n\")[:-1]\n","test_pairs = []\n","for line in lines:\n","    inp, targ = line.split(\"\\t\")\n","    targ = \"[start] \" + targ + \" [end]\"\n","    test_pairs.append((inp, targ))\n","\n","for _ in range(5):\n","    print(random.choice(test_pairs))\n","\n","# random.shuffle(test_pairs)\n","num_test_samples = len(test_pairs)\n","test_pairs = test_pairs[: num_test_samples]\n","print(f\"{len(test_pairs)} test pairs\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1CpqzqNaa-w0"},"source":["## Vectorizing the text data\n","\n","We'll use two instances of the `TextVectorization` layer to vectorize the text\n","data (one for English and one for Spanish),\n","that is to say, to turn the original strings into integer sequences\n","where each integer represents the index of a word in a vocabulary.\n","\n","The English layer will use the default string standardization (strip punctuation characters)\n","and splitting scheme (split on whitespace), while\n","the Spanish layer will use a custom standardization, where we add the character\n","`\"¿\"` to the set of punctuation characters to be stripped.\n","\n","Note: in a production-grade machine translation model, I would not recommend\n","stripping the punctuation characters in either language. Instead, I would recommend turning\n","each punctuation character into its own token,\n","which you could achieve by providing a custom `split` function to the `TextVectorization` layer."]},{"cell_type":"code","metadata":{"id":"aQUtMJPhIv1G","executionInfo":{"status":"aborted","timestamp":1631690351485,"user_tz":-180,"elapsed":21,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["# strip_chars = string.punctuation + \"¿\"\n","strip_chars = \"[a-zA-Z]|\\d+|[٠١٢٣٤٥٦٧٨٩]|[.#،<>@,\\\\-_”“٪ًَ]\"\n","\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n","\n","vocab_size = 6000\n","sequence_length = 20\n","batch_size = 64\n","\n","\n","def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)\n","    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n","\n","\n","# inp_vectorization = TextVectorization(\n","#     max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n","# )\n","targ_vectorization = TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length + 1,\n","    standardize=custom_standardization,\n",")\n","# train_eng_texts = [pair[0] for pair in train_pairs]\n","train_spa_texts = [pair[1] for pair in train_pairs]\n","# inp_vectorization.adapt(train_eng_texts)\n","targ_vectorization.adapt(train_spa_texts)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CPoQesZea-w0","executionInfo":{"status":"aborted","timestamp":1631690351487,"user_tz":-180,"elapsed":23,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["# # strip_chars = string.punctuation + \"¿\"\n","# strip_chars = \"[a-zA-Z]|\\d+|[٠١٢٣٤٥٦٧٨٩]|[.#،<>@,\\\\-_”“٪ًَ]\"\n","\n","# strip_chars = strip_chars.replace(\"[\", \"\")\n","# strip_chars = strip_chars.replace(\"]\", \"\")\n","\n","# vocab_size = 5000\n","# sequence_length = 20\n","# batch_size = 64\n","\n","\n","# def custom_standardization(input_string):\n","#     lowercase = tf.strings.lower(input_string)\n","#     return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n","\n","\n","# inp_vectorization = TextVectorization(\n","#     max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n","# )\n","# targ_vectorization = TextVectorization(\n","#     max_tokens=vocab_size,\n","#     output_mode=\"int\",\n","#     output_sequence_length=sequence_length + 1,\n","#     standardize=custom_standardization,\n","# )\n","# train_eng_texts = [pair[0] for pair in train_pairs]\n","# train_spa_texts = [pair[1] for pair in train_pairs]\n","# inp_vectorization.adapt(train_eng_texts)\n","# targ_vectorization.adapt(train_spa_texts)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8enIADXMa-w1"},"source":["Next, we'll format our datasets.\n","\n","At each training step, the model will seek to predict target words N+1 (and beyond)\n","using the source sentence and the target words 0 to N.\n","\n","As such, the training dataset will yield a tuple `(inputs, targets)`, where:\n","\n","- `inputs` is a dictionary with the keys `encoder_inputs` and `decoder_inputs`.\n","`encoder_inputs` is the vectorized source sentence and `encoder_inputs` is the target sentence \"so far\",\n","that is to say, the words 0 to N used to predict word N+1 (and beyond) in the target sentence.\n","- `target` is the target sentence offset by one step:\n","it provides the next words in the target sentence -- what the model will try to predict."]},{"cell_type":"code","metadata":{"id":"dKmCVA5Ka-w2","executionInfo":{"status":"aborted","timestamp":1631690351487,"user_tz":-180,"elapsed":23,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["\n","def format_dataset(inp, targ):\n","    inp = inp_vectorization(inp)\n","    targ = targ_vectorization(targ)\n","    return ({\"encoder_inputs\": inp, \"decoder_inputs\": targ[:, :-1],}, targ[:, 1:])\n","\n","\n","def make_dataset(pairs):\n","    inp_texts, targ_texts = zip(*pairs)\n","    inp_texts = list(inp_texts)\n","    targ_texts = list(targ_texts)\n","    dataset = tf.data.Dataset.from_tensor_slices((inp_texts, targ_texts))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset)\n","    return dataset.shuffle(248).prefetch(16).cache()\n","\n","\n","train_ds = make_dataset(train_pairs)\n","val_ds = make_dataset(val_pairs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U6ZQPBz4a-w3"},"source":["Let's take a quick look at the sequence shapes\n","(we have batches of 64 pairs, and all sequences are 20 steps long):"]},{"cell_type":"code","metadata":{"id":"PkVSFSxBa-w3","executionInfo":{"status":"aborted","timestamp":1631690351488,"user_tz":-180,"elapsed":24,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["for inputs, targets in train_ds.take(1):\n","    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n","    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n","    print(f\"targets.shape: {targets.shape}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GQ6z9fVea-w3"},"source":["## Building the model\n","\n","Our sequence-to-sequence Transformer consists of a `TransformerEncoder`\n","and a `TransformerDecoder` chained together. To make the model aware of word order,\n","we also use a `PositionalEmbedding` layer.\n","\n","The source sequence will be pass to the `TransformerEncoder`,\n","which will produce a new representation of it.\n","This new representation will then be passed\n","to the `TransformerDecoder`, together with the target sequence so far (target words 0 to N).\n","The `TransformerDecoder` will then seek to predict the next words in the target sequence (N+1 and beyond).\n","\n","A key detail that makes this possible is causal masking\n","(see method `get_causal_attention_mask()` on the `TransformerDecoder`).\n","The `TransformerDecoder` sees the entire sequences at once, and thus we must make\n","sure that it only uses information from target tokens 0 to N when predicting token N+1\n","(otherwise, it could use information from the future, which would\n","result in a model that cannot be used at inference time)."]},{"cell_type":"code","metadata":{"id":"3jHRvQaHa-w3","executionInfo":{"status":"aborted","timestamp":1631690351489,"user_tz":-180,"elapsed":25,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["\n","class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super(TransformerEncoder, self).__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\",kernel_regularizer=l2(0.1)), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n","        attention_output = self.attention(\n","            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n","        )\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","\n","\n","class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n","        super(PositionalEmbedding, self).__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=vocab_size, output_dim=embed_dim\n","        )\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=embed_dim\n","        )\n","        self.sequence_length = sequence_length\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","\n","\n","class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n","        super(TransformerDecoder, self).__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.latent_dim = latent_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(latent_dim, activation=\"relu\",kernel_regularizer=l2(0.1)), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","        if mask is not None:\n","            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n","            padding_mask = tf.minimum(padding_mask, causal_mask)\n","\n","        attention_output_1 = self.attention_1(\n","            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n","        )\n","        out_1 = self.layernorm_1(inputs + attention_output_1)\n","\n","        attention_output_2 = self.attention_2(\n","            query=out_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","            attention_mask=padding_mask,\n","        )\n","        out_2 = self.layernorm_2(out_1 + attention_output_2)\n","\n","        proj_output = self.dense_proj(out_2)\n","        return self.layernorm_3(out_2 + proj_output)\n","\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n","            axis=0,\n","        )\n","        return tf.tile(mask, mult)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WCD0jPjqa-w4"},"source":["Next, we assemble the end-to-end model."]},{"cell_type":"code","metadata":{"id":"PaxOj5PWa-w5","executionInfo":{"status":"aborted","timestamp":1631690351490,"user_tz":-180,"elapsed":26,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["from keras.regularizers import l2\n","\n","embed_dim = 256\n","latent_dim = 2048\n","num_heads = 8\n","\n","encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n","encoder = keras.Model(encoder_inputs, encoder_outputs)\n","\n","decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n","encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n","x = layers.Dropout(0.5)(x)\n","decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\",kernel_regularizer=l2(0.001))(x)\n","decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n","\n","decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n","transformer = keras.Model(\n","    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9dFxoe3ma-w5"},"source":["## Training our model\n","\n","We'll use accuracy as a quick way to monitor training progress on the validation data.\n","Note that machine translation typically uses BLEU scores as well as other metrics, rather than accuracy.\n","\n","Here we only train for 1 epoch, but to get the model to actually converge\n","you should train for at least 30 epochs."]},{"cell_type":"code","metadata":{"id":"8tGD8iZLa-w6","executionInfo":{"status":"aborted","timestamp":1631690351490,"user_tz":-180,"elapsed":25,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["epochs = 30  # This should be at least 30 for convergence\n","my_callbacks = [\n","    tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=2),\n","]\n","transformer.summary()\n","transformer.compile(\n","    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",")\n","\n","history  = transformer.fit(train_ds, epochs=epochs, validation_data=val_ds,callbacks=my_callbacks)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AoxqTnCF_2tK","executionInfo":{"status":"aborted","timestamp":1631690351491,"user_tz":-180,"elapsed":26,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["import matplotlib.pyplot as plt\n","\n","s, (at, al) = plt.subplots(2,1)\n","at.plot(history.history['accuracy'], c= 'b')\n","at.plot(history.history['val_accuracy'], c='r')\n","at.set_title('model accuracy')\n","at.set_ylabel('accuracy')\n","at.set_xlabel('epoch')\n","at.legend(['train', 'val'], loc='upper left')\n","\n","al.plot(history.history['loss'], c='m')\n","al.plot(history.history['val_loss'], c='c')\n","al.set_title('model loss')\n","al.set_ylabel('loss')\n","al.set_xlabel('epoch')\n","al.legend(['train', 'val'], loc = 'upper left')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SL-vwLH1a-w6"},"source":["## Decoding test sentences\n","\n","Finally, let's demonstrate how to translate brand new English sentences.\n","We simply feed into the model the vectorized English sentence\n","as well as the target token `\"[start]\"`, then we repeatedly generated the next token, until\n","we hit the token `\"[end]\"`."]},{"cell_type":"code","metadata":{"id":"okeujBqTa-w6","executionInfo":{"status":"aborted","timestamp":1631690351491,"user_tz":-180,"elapsed":26,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["spa_vocab = targ_vectorization.get_vocabulary()\n","spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n","max_decoded_sentence_length = 20\n","\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = inp_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = targ_vectorization([decoded_sentence])[:, :-1]\n","        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n","\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = spa_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","\n","        if sampled_token == \"[end]\":\n","            break\n","    return decoded_sentence\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aUHJihnVKPbt","executionInfo":{"status":"aborted","timestamp":1631690351492,"user_tz":-180,"elapsed":27,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["from nltk.translate.bleu_score import corpus_bleu\n","from nltk.translate.bleu_score import sentence_bleu\n","from nltk.translate import bleu\n","predicted_list = []\n","test_inp_texts = [pair[0] for pair in test_pairs]\n","test_targ_texts = [pair[1] for pair in test_pairs]\n","\n","score_list = []\n","score_list_2 = []\n","score_list_3 = []\n","score_list_4 = []\n","# from nltk.translate.bleu_score import SmoothingFunction\n","# print(len(test_inp_texts))\n","# print(len(test_targ_texts))\n","# def bleu_score():\n","\n","for i,j in zip(test_inp_texts,test_targ_texts):\n","\n","  # input_sentence = test_inp_texts\n","    # print(input_sentence[i])\n","  translated = decode_sequence(i)\n","  # print([j.split()],'salam test ')\n","  # print(translated,'salam test ')\n","  # predicted = list(translated.split(\",\"))\n","  # score = sentence_bleu(i, translated, weights=(1, 0, 0, 0))\n","  score_1 = sentence_bleu([j.split()], translated.split(), weights=(1, 0, 0, 0))\n","  score_2 = sentence_bleu([j.split()], translated.split(),weights=(0.5, 0.5, 0, 0))\n","  score_3 = sentence_bleu([j.split()], translated.split(), weights=(0.33, 0.33, 0.33, 0))\n","  score_4 = sentence_bleu([j.split()], translated.split(), weights=(0.25, 0.25, 0.25, 0.25))\n","   \n","  # score_11 = bleu([j.split()], translated.split(),  weights=(1, 0, 0, 0))\n","  # score_22 = bleu([j.split()], translated.split(), weights=(0, 1, 0, 0))\n","  # score_33 = bleu([j.split()], translated.split(), weights=(0, 0, 1, 0))\n","  # score_44 = bleu([j.split()], translated.split(), weights=(0, 0, 0, 1))\n","  score_list.append(score_1)\n","  score_list_2.append(score_2)\n","  score_list_3.append(score_3)\n","  score_list_4.append(score_4)\n","  predicted_list.append(translated)\n","  print(\"Input:\",i,\"\\n Actual\",j,\"\\n Predicted\",translated)\n","\n","  print('------- Cumulative n-grams ----')\n","  print(\"blue 1-gram : \",score_1,\"\\n\",\"blue 2-gram : \",score_2,\"\\n\",\"blue 3-gram : \",score_3,\"\\n\",\"blue 4-gram : \",score_4,\"\\n\")\n","  # print('------- Individual n-grams ----')\n","  # print(\"blue 1-gram : \",score_11,\"\\n\",\"blue 2-gram : \",score_22,\"\\n\",\"blue 3-gram : \",score_33,\"\\n\",\"blue 4-gram : \",score_44,\"\\n\")\n","\n","avg = sum(score_list) / len(score_list)\n","# print(\"1\",score_list)\n","avg_2 = sum(score_list_2) / len(score_list_2)\n","# print(\"2\",score_list_2)\n","avg_3 = sum(score_list_3) / len(score_list_3)\n","# print(\"3\",score_list_3)\n","avg_4 = sum(score_list_4) / len(score_list_4)\n","# print(\"4\",score_list_4)\n","\n","print(\"Average of the list 1-gram=\", round(avg, 2)*100,\"Average of the list 2-gram=\", round(avg_2, 2)*100,\"Average of the list 3-gram=\", \n","      round(avg_3, 2)*100,\"Average of the list 4-gram=\", round(avg_4, 2)*100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mJgFqqDDer6v","executionInfo":{"status":"aborted","timestamp":1631690351492,"user_tz":-180,"elapsed":26,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["import pandas as pd\n","df = pd.DataFrame(columns=['Input','Actual','Predicated'])\n","\n","dict = {'Input': test_inp_texts, 'Actual': test_targ_texts, 'Predicated': predicted_list,\n","        'Blue 1-gram': score_list,'Blue 2-gram': score_list_2,'Blue 3-gram': score_list_3,'Blue 4-gram': score_list_4} \n","# header_list = ['input','actual','predicated']\n","df = pd.DataFrame(dict)\n","df = df.to_csv(\"withNotHS_exp3183_6000.csv\",index=False)\n","\n","# saving the dataframe\n","!cp withNotHS_exp3183_6000.csv \"/content/drive/MyDrive/MasterThesis/paraphrasing/outputs\"\n","out= pd.read_csv('/content/drive/MyDrive/MasterThesis/paraphrasing/outputs/withNotHS_exp3183_6000.csv')\n","out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cZCUkO4AzSIf","executionInfo":{"status":"aborted","timestamp":1631690351493,"user_tz":-180,"elapsed":27,"user":{"displayName":"Do salam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgP0GyFxHH-3NAPSZ_ethp4uMqaN8pUWhOxcO0NDw=s64","userId":"03587359084159229589"}}},"source":["import pandas as pd\n","# df = pd.DataFrame(columns=['Input','Actual','Predicated'])\n","\n","# dict1 = {'Input': test_inp_texts} \n","# dict2 = { 'Actual': test_targ_texts} \n","dict3 = {'Predicated': predicted_list} \n","# header_list = ['input','actual','predicated']\n","# df1= pd.DataFrame(dict1)\n","# df2 = pd.DataFrame(dict2)\n","df3 = pd.DataFrame(dict3)\n","# df1 = df1.to_csv(\"inp3183_6000.txt\" ,index=False,header=False)\n","# df2 = df2.to_csv(\"act3183_6000.txt\" ,index=False,header=False)\n","df3 = df3.to_csv(\"pred3183_6000.txt\",index=False,header=False)\n","\n","# saving the dataframe\n","# !cp inp.txt \"/content/drive/MyDrive/MasterThesis/paraphrasing/outputs\"\n","# !cp act.txt \"/content/drive/MyDrive/MasterThesis/paraphrasing/outputs\"\n","!cp pred3183_6000.txt \"/content/drive/MyDrive/MasterThesis/paraphrasing/outputs\"\n","# out= pd.read_csv('/content/drive/MyDrive/MasterThesis/paraphrasing/outputs/inp.txt')\n","# out1= pd.read_csv('/content/drive/MyDrive/MasterThesis/paraphrasing/outputs/act.txt')\n","out2= pd.read_csv('/content/drive/MyDrive/MasterThesis/paraphrasing/outputs/pred3183_6000.txt')\n","# print(out)\n","# print(out1)\n","print(out2)"],"execution_count":null,"outputs":[]}]}